{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDDS18-GTFS/ddds.18.capstone/blob/main/GTFS_ABQ_load_pickle_set_params_merge_fix_set_anomalies_v9_1ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ABC note 250726-1715: I have some concerns about the anomaly placement, but I also haven't worked with a multi-day dataset previously, so I need to do some more testing.\n"
      ],
      "metadata": {
        "id": "9F7Kj_3XA5uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Libraries"
      ],
      "metadata": {
        "id": "KI4QbV7NQx9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_t8RdRcMqIZ"
      },
      "outputs": [],
      "source": [
        "#Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# # Install if needed\n",
        "# !pip install -q ipywidgets\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "from shapely.geometry import Point, LineString\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "from folium import Map, FeatureGroup, CircleMarker, PolyLine, Marker, Icon, LayerControl\n",
        "from matplotlib import colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import folium\n",
        "\n",
        "!pip install psycopg2-binary pandas sqlalchemy\n",
        "\n",
        "import psycopg2\n",
        "from sqlalchemy import create_engine\n",
        "import os\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Database connection parameters\n",
        "DB_NAME = \"abq-transit-db\"\n",
        "\n",
        "# You'll need to store your database password/paramenters in Colab Secrets\n",
        "DB_HOST = userdata.get('DB_HOST_2')\n",
        "DB_PORT = userdata.get('DB_PORT')\n",
        "DB_PASSWORD = userdata.get('DB_PASSWORD')\n",
        "DB_USER = userdata.get(\"DB_USER\")\n",
        "\n",
        "\n",
        "# Create connection string\n",
        "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "engine = create_engine(connection_string)"
      ],
      "metadata": {
        "id": "gOr5UyCpmHKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Parameters for Anomaly Detection\n",
        "# This section sets threshholds for classifying anomalous transit events.\n",
        "\n",
        "# --- Use meters for user input ---\n",
        "# The jump_thresh_m_widget creates a sliding bar for selecting how much distance there must be between\n",
        "# two consecutive positions for a single vehicle to have gone 'unusually far' or 'jumped'.\n",
        "jump_thresh_m_widget = widgets.IntSlider(\n",
        "    value=500,\n",
        "    min=50,\n",
        "    max=2000,\n",
        "    step=50,\n",
        "    description='Jump Distance',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# Other anomaly widgets\n",
        "# The disappear_thresh_widget creates a sliding bar for selecting how many consecutive seconds  must pass\n",
        "# without a vehicle transmitting data before we consider that vehicle to have been gone 'unusually long'.\n",
        "disappear_thresh_widget = widgets.IntSlider(value=300, min=60, max=1800, step=30, description='Time Gap')\n",
        "\n",
        "# The min_jump_widget creates a sliding bar for selecting how many 'jumps' a vehicle must make before\n",
        "# we consider the jumping to be 'unusual'.\n",
        "# SUGGESTION: can we make this based off of jump points as a percentage of total recorded points a required number of consecutive jumps?\n",
        "# As we use more data, any set value of total jumps will become more and more easily met.\n",
        "min_jump_widget = widgets.IntSlider(value=2, min=1, max=10, step=1, description='Min Jumps')\n",
        "\n",
        "# The speed_thresh_widget creates a sliding bar for selecting how fast a vihicke must be going for us\n",
        "# to consider that vehicle as 'unusually fast'.\n",
        "speed_thresh_widget = widgets.IntSlider(value=70, min=10, max=100, step=5, description='Speed')\n",
        "\n",
        "# motion to depricate this widget\n",
        "reversal_thresh_widget = widgets.IntSlider(value=120, min=60, max=180, step=5, description='Heading Î”Â°')\n",
        "\n",
        "# --- Stuck vehicle detection ---\n",
        "# The stuck_speed_widget creates a sliding bar for selecting how slow a vehicle must be going\n",
        "# for us to consider it as effectively unmoving.\n",
        "stuck_speed_widget = widgets.FloatSlider(\n",
        "    value=1.0,\n",
        "    min=0.0,\n",
        "    max=5.0,\n",
        "    step=0.1,\n",
        "    description='Stuck Speed',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# The stuck_window_widget creates a sliding bar for selecting how many consecutive Stuck Speed\n",
        "# events a vehicle must have before it is considered to have been still for an unusually long period.\n",
        "stuck_window_widget = widgets.IntSlider(\n",
        "    value=4,\n",
        "    min=1,\n",
        "    max=20,\n",
        "    step=1,\n",
        "    description='Window Size',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# --- Repeated point tolerance ---\n",
        "# motion to depricate this widget\n",
        "repeat_tolerance_widget = widgets.FloatLogSlider(\n",
        "    value=1e-5,\n",
        "    base=10,\n",
        "    min=-7,  # 1e-7\n",
        "    max=-3,  # 1e-3\n",
        "    step=0.1,\n",
        "    description='Repeat Tolerance',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# --- Early appearance margin ---\n",
        "# motion to depricate this widget\n",
        "early_margin_widget = widgets.IntSlider(\n",
        "    value=30,\n",
        "    min=0,\n",
        "    max=600,\n",
        "    step=10,\n",
        "    description='Early Margin',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# --- Off-route buffer ---\n",
        "# The offroute_buffer_widget creates a sliding bar for selecting how many meters\n",
        "# a vehicle must be off of its expected route before we actually label it as off route\n",
        "offroute_buffer_widget = widgets.IntSlider(\n",
        "    value=50,\n",
        "    min=10,\n",
        "    max=500,\n",
        "    step=10,\n",
        "    description='Off-Route Buffer',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# Labels\n",
        "jump_label = widgets.Label(value=\"(meters)\")\n",
        "disappear_label = widgets.Label(value=\"(seconds)\")\n",
        "min_jump_label = widgets.Label(value=\"(count)\")\n",
        "speed_label = widgets.Label(value=\"(mph)\")\n",
        "reversal_label = widgets.Label(value=\"(degrees of reversal)\")\n",
        "stuck_speed_label = widgets.Label(value=\"(mph)\")\n",
        "stuck_window_label = widgets.Label(value=\"(frames in rolling window)\")\n",
        "repeat_tolerance_label = widgets.Label(value=\"(decimal degrees)\")\n",
        "early_margin_label = widgets.Label(value=\"(seconds)\")\n",
        "offroute_buffer_label = widgets.Label(value=\"(meters)\")\n",
        "\n",
        "# Assemble UI layout\n",
        "slider_widgets = widgets.VBox([\n",
        "    widgets.HBox([jump_thresh_m_widget, jump_label]),\n",
        "    widgets.HBox([disappear_thresh_widget, disappear_label]),\n",
        "    widgets.HBox([min_jump_widget, min_jump_label]),\n",
        "    widgets.HBox([speed_thresh_widget, speed_label]),\n",
        "    widgets.HBox([reversal_thresh_widget, reversal_label]),\n",
        "    widgets.HBox([stuck_speed_widget, stuck_speed_label]),\n",
        "    widgets.HBox([stuck_window_widget, stuck_window_label]),\n",
        "    widgets.HBox([repeat_tolerance_widget, repeat_tolerance_label]),\n",
        "    widgets.HBox([early_margin_widget, early_margin_label]),\n",
        "    widgets.HBox([offroute_buffer_widget, offroute_buffer_label]),\n",
        "])\n",
        "\n",
        "# Button and save logic\n",
        "submit_button = widgets.Button(description=\"Save Parameters\", button_style='primary')\n",
        "anomaly_params = {}\n",
        "\n",
        "def save_params(b):\n",
        "    '''\n",
        "    save_params displays widgets that allow you to set up and save anomaly detection threshholds.\n",
        "    '''\n",
        "    clear_output(wait=True)\n",
        "    display(slider_widgets, submit_button)\n",
        "\n",
        "    global anomaly_params\n",
        "    jump_m = jump_thresh_m_widget.value\n",
        "    jump_deg = jump_m / 111000  # Convert meters â†’ degrees\n",
        "\n",
        "    anomaly_params = {\n",
        "        \"JUMP_DISTANCE_THRESHOLD\": jump_deg,\n",
        "        \"JUMP_DISTANCE_METERS\": jump_m,\n",
        "        \"DISAPPEARANCE_TIME_THRESHOLD\": disappear_thresh_widget.value,\n",
        "        \"MIN_JUMP_COUNT_PER_VEHICLE\": min_jump_widget.value,\n",
        "        \"SPEED_LIMIT_MPH\": speed_thresh_widget.value,\n",
        "        \"REVERSAL_HEADING_THRESHOLD\": reversal_thresh_widget.value,\n",
        "        \"STUCK_SPEED_MPH\": stuck_speed_widget.value,\n",
        "        \"STUCK_WINDOW_SIZE\": stuck_window_widget.value,\n",
        "        \"REPEATED_COORD_TOLERANCE\": repeat_tolerance_widget.value,\n",
        "        \"EARLY_APPEARANCE_MARGIN_SEC\": early_margin_widget.value,\n",
        "        \"OFF_ROUTE_BUFFER_M\": offroute_buffer_widget.value,\n",
        "    }\n",
        "\n",
        "    print(\"âœ… Anomaly detection parameters set:\")\n",
        "    for k, v in anomaly_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "submit_button.on_click(save_params)\n",
        "\n",
        "# Display interface\n",
        "display(slider_widgets, submit_button)\n"
      ],
      "metadata": {
        "id": "s1VgiJSwhqyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOAD PICKLE"
      ],
      "metadata": {
        "id": "RzTAQJWF6aUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell should be used when loading pickle files as opposed to using the database\n",
        "#import pickle\n",
        "\n",
        "# Load a pickle file of a dataframe which contains our historic transit data.\n",
        "#with open(\"captures.p\", \"rb\") as f:\n",
        "#    captured_data = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "hkgJx5g06YAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with engine.connect() as connection:\n",
        "    captured_data = pd.read_sql_query(\n",
        "        '''\n",
        "        SELECT\n",
        "          id, snapshot_id, msg_time, timestamp_collected, vehicle_id, location, heading, speed_mph,\n",
        "          route_short_name, trip_id, next_stop_id, next_stop_id, next_stop_name,\n",
        "          next_stop_sched_time\n",
        "        FROM\n",
        "          vehicle_snapshots\n",
        "        WHERE\n",
        "          DATE(\"timestamp_collected\") = CURRENT_DATE - INTERVAL '1 DAY'\n",
        "        ORDER BY\n",
        "          timestamp_collected DESC;\n",
        "        ''', connection)\n",
        "\n",
        "captured_data.head()"
      ],
      "metadata": {
        "id": "E2qOSTthmksq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Validation"
      ],
      "metadata": {
        "id": "YO8DwgxF-TES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd_df = captured_data\n",
        "cd_df.size"
      ],
      "metadata": {
        "id": "fqGP4gVT8nTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd_df.head(20)"
      ],
      "metadata": {
        "id": "Fitq360CCFkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Display full list of column names\n",
        "# print(\"Number of columns:\", len(cd_df.columns))\n",
        "# print(\"Column names:\\n\", cd_df.columns.tolist())"
      ],
      "metadata": {
        "id": "B2n30JXU9ibs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd_df.head()"
      ],
      "metadata": {
        "id": "mTtHlxby9Xov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to datetime\n",
        "cd_df['timestamp_collected'] = pd.to_datetime(cd_df['timestamp_collected'], errors='coerce')\n",
        "\n",
        "# 1. Shape and Column Overview\n",
        "print(\"Shape:\", cd_df.shape)\n",
        "print(\"Columns:\", cd_df.columns.tolist())\n",
        "\n",
        "# 2. Null/Missing Value Counts\n",
        "print(\"\\nMissing Values:\\n\", cd_df.isnull().sum())\n",
        "\n",
        "# 3. Duplicate Detection\n",
        "duplicates = cd_df.duplicated().sum()\n",
        "print(\"\\nDuplicate rows:\", duplicates)\n",
        "\n",
        "# 4. Timestamp Range\n",
        "print(\"\\nTimestamp Range:\")\n",
        "print(\"Min:\", cd_df['timestamp_collected'].min())\n",
        "print(\"Max:\", cd_df['timestamp_collected'].max())\n",
        "print(\"Duration:\", cd_df['timestamp_collected'].max() - cd_df['timestamp_collected'].min())\n",
        "\n",
        "# 5. Latitude/Longitude Range\n",
        "print(\"\\nLatitude Range:\", cd_df['latitude'].min(), \"to\", cd_df['latitude'].max())\n",
        "print(\"Longitude Range:\", cd_df['longitude'].min(), \"to\", cd_df['longitude'].max())\n",
        "\n",
        "# 6. Speed Summary\n",
        "print(\"\\nSpeed Summary:\\n\", cd_df['speed_mph'].describe())\n"
      ],
      "metadata": {
        "id": "UYDyndY5947D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the histogram and get the counts and bin edges\n",
        "# counts, bins, patches = plt.hist(cd_df['longitude'], bins=2, rwidth=0.8)\n",
        "\n",
        "# # Add labels and title\n",
        "# plt.xlabel(\"Value\")\n",
        "# plt.ylabel(\"Count\")\n",
        "# plt.title(\"Histogram of Values with Counts per Bin\")\n",
        "\n",
        "# # Display the plot\n",
        "# plt.show()\n",
        "\n",
        "# # You can also access the counts and bin edges directly:\n",
        "# print(\"Bin counts:\", counts)\n",
        "# print(\"Bin edges:\", bins)"
      ],
      "metadata": {
        "id": "ASrqqPrXeJMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the histogram and get the counts and bin edges\n",
        "# counts, bins, patches = plt.hist(cd_df['latitude'], bins=2, rwidth=0.8)\n",
        "\n",
        "# # Add labels and title\n",
        "# plt.xlabel(\"Value\")\n",
        "# plt.ylabel(\"Count\")\n",
        "# plt.title(\"Histogram of Values with Counts per Bin\")\n",
        "\n",
        "# # Display the plot\n",
        "# plt.show()\n",
        "\n",
        "# # You can also access the counts and bin edges directly:\n",
        "# print(\"Bin counts:\", counts)\n",
        "# print(\"Bin edges:\", bins)"
      ],
      "metadata": {
        "id": "KujXP4wAfOLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows where 'longitude' and 'latitude' have the same value\n",
        "rows_with_same_values = cd_df[cd_df['longitude'] == cd_df['latitude']]\n",
        "rows_without_same_values = cd_df[cd_df['longitude'] != cd_df['latitude']]\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(rows_with_same_values)"
      ],
      "metadata": {
        "id": "_9qzN8yjgGsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rows_without_same_values.shape"
      ],
      "metadata": {
        "id": "hslx96cbgcSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd_df.shape"
      ],
      "metadata": {
        "id": "eQAsxnFCg58h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####*.v8.2 update\n",
        "We're removing the off-duty rows where Lat = Long = 0"
      ],
      "metadata": {
        "id": "-SIkutvnhHGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd_df_copy = cd_df.copy()\n",
        "cd_df_copy.shape"
      ],
      "metadata": {
        "id": "IsEX9SDlhSGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd_df = cd_df[cd_df['longitude'] != cd_df['latitude']]\n",
        "cd_df.shape"
      ],
      "metadata": {
        "id": "9-ktOsTNhdFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##âœ… Overall Status\n",
        "Your dataset appears valid and cleanly structured for analysis. You can safely proceed with anomaly detection, with only minor issues to address or monitor.\n",
        "\n",
        "| Category            | Result                               | Notes                                                                          |\n",
        "| ------------------- | ------------------------------------ | ------------------------------------------------------------------------------ |\n",
        "| **Shape**           | (1,096,540 rows, 12 columns)         | Substantial volume, well-sized for analysis                                    |\n",
        "| **Missing Values**  | 0 missing in any column              | âœ… Excellent data integrity                                                     |\n",
        "| **Duplicates**      | 0 duplicate rows                     | âœ… Clean snapshot logic                                                         |\n",
        "| **Timestamp Range** | \\~1.6 days                           | âš ï¸ Slightly under 2 days; may be due to gaps or partial logging                |\n",
        "| **Latitude Range**  | `0.0` to `35.41`                     | âš ï¸ `0.0` indicates corrupted or failed GPS points â€” needs filtering            |\n",
        "| **Longitude Range** | `-106.79` to `0.0`                   | âš ï¸ Same issue â€” longitudes of `0.0` are invalid (off the west coast of Africa) |\n",
        "| **Speed (mph)**     | Mean: `4.2`, Max: `80`, 75% at `0.0` | âš ï¸ Excessive zero speeds â†’ vehicles idle or unreported? Worth mapping          |\n",
        "| **Outliers**        | Max speed = `80 mph`                 | ðŸ” Borderline â€” worth flagging for manual inspection                           |\n"
      ],
      "metadata": {
        "id": "rEzPsYyx-Dsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Some preliminary prep for analysis\n",
        "\n",
        "# # Remove invalid GPS coordinates\n",
        "# cd_df_valid = cd_df[(cd_df['latitude'] > 30) & (cd_df['longitude'] < -100)]\n",
        "\n",
        "# # Optional: filter out idle snapshots (speed = 0) if focusing on movement\n",
        "# cd_df_moving = cd_df_valid[cd_df_valid['speed_mph'] > 0]\n"
      ],
      "metadata": {
        "id": "ajuwC9pU-XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert code for snapshot to use Pickle"
      ],
      "metadata": {
        "id": "e9zMwStn-n12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_cd_df(df, start_time=None, end_time=None, routes=None, vehicles=None, bounds=None):\n",
        "    '''\n",
        "    Filters a dataframe(df) such that:\n",
        "    - the \"timestamp_collected\" column's entries are greater than or equal to the start_time\n",
        "    - the \"timestamp_collected\" column's entries are less than or equal to the end_time\n",
        "    - the \"route_short_name\" column's entries are in routes\n",
        "    - the \"vehicle_id\" column's entries are in vehicles\n",
        "\n",
        "    Returns the filtered dataframe.\n",
        "    '''\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    if start_time:\n",
        "        df_filtered = df_filtered[df_filtered['timestamp_collected'] >= pd.to_datetime(start_time)]\n",
        "    if end_time:\n",
        "        df_filtered = df_filtered[df_filtered['timestamp_collected'] <= pd.to_datetime(end_time)]\n",
        "    if routes:\n",
        "        df_filtered = df_filtered[df_filtered['route_short_name'].isin(routes)]\n",
        "    if vehicles:\n",
        "        df_filtered = df_filtered[df_filtered['vehicle_id'].isin(vehicles)]\n",
        "    if bounds:\n",
        "        lat_min, lat_max, lon_min, lon_max = bounds\n",
        "        df_filtered = df_filtered[\n",
        "            (df_filtered['latitude'] >= lat_min) & (df_filtered['latitude'] <= lat_max) &\n",
        "            (df_filtered['longitude'] >= lon_min) & (df_filtered['longitude'] <= lon_max)\n",
        "        ]\n",
        "\n",
        "    return df_filtered\n"
      ],
      "metadata": {
        "id": "3cUbW5rt_lS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Widgets ---\n",
        "# route_widget lets you select which routes you'd like to like to plot in folium from a menu.\n",
        "route_widget = widgets.SelectMultiple(\n",
        "    options=sorted(cd_df['route_short_name'].unique()),\n",
        "    description='Routes',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "# vehicle_widget lets you select which vehicles' data you'd like to plot in folium from a menu.\n",
        "vehicle_widget = widgets.SelectMultiple(\n",
        "    options=sorted(cd_df['vehicle_id'].unique()),\n",
        "    description='Vehicles',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "# start_widget lets you select the earliest date you'd like to use data from.\n",
        "start_widget = widgets.DatePicker(\n",
        "    description='Start Date',\n",
        "    value=cd_df['timestamp_collected'].min().date()\n",
        ")\n",
        "# end_widget lets you select the latest date you'd like to use data from.\n",
        "end_widget = widgets.DatePicker(\n",
        "    description='End Date',\n",
        "    value=cd_df['timestamp_collected'].max().date()\n",
        ")\n",
        "\n",
        "# hour_slider creates a set of sliding bars for selecting the earliest and latest\n",
        "# point of the day that you'd like to use data from.\n",
        "hour_slider = widgets.IntRangeSlider(\n",
        "    value=[0, 23],\n",
        "    min=0,\n",
        "    max=23,\n",
        "    step=1,\n",
        "    description='Hour Range',\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "# Makes a button, that will later be used to apply filters.\n",
        "filter_button = widgets.Button(description='Apply Filters', button_style='primary')\n",
        "\n",
        "# --- Global to store result ---\n",
        "cd_df_filtered = pd.DataFrame()\n",
        "\n",
        "# --- Filtering callback ---\n",
        "def apply_filters(b):\n",
        "    '''\n",
        "    Uses previously defined widgets to filter a dataframe of historical transit data\n",
        "    by rout_short_name, vehicle_id, and timestamp collected.\n",
        "    Prints the number of filtered rows and displays the first 10 rows of the filtered dataframe.\n",
        "    '''\n",
        "    global cd_df_filtered\n",
        "    clear_output(wait=True)\n",
        "    display(route_widget, vehicle_widget, start_widget, end_widget, hour_slider, filter_button)\n",
        "\n",
        "    df = cd_df.copy()\n",
        "\n",
        "    # Apply filters\n",
        "    if route_widget.value:\n",
        "        df = df[df['route_short_name'].isin(route_widget.value)]\n",
        "    if vehicle_widget.value:\n",
        "        df = df[df['vehicle_id'].isin(vehicle_widget.value)]\n",
        "\n",
        "    start_dt = pd.to_datetime(start_widget.value)\n",
        "    end_dt = pd.to_datetime(end_widget.value) + pd.Timedelta(days=1)\n",
        "    df = df[(df['timestamp_collected'] >= start_dt) &\n",
        "            (df['timestamp_collected'] < end_dt)]\n",
        "\n",
        "    hr_start, hr_end = hour_slider.value\n",
        "    df = df[(df['timestamp_collected'].dt.hour >= hr_start) &\n",
        "            (df['timestamp_collected'].dt.hour <= hr_end)]\n",
        "\n",
        "    # Store result globally\n",
        "    cd_df_filtered = df\n",
        "\n",
        "    print(f\"âœ… Filtered {len(df)} rows.\")\n",
        "    display(df.head(10))\n",
        "\n",
        "# Displays a filter button that calls the 'apply_filters' function\n",
        "filter_button.on_click(apply_filters)\n",
        "\n",
        "# --- Display UI ---\n",
        "display(route_widget, vehicle_widget, start_widget, end_widget, hour_slider, filter_button)\n"
      ],
      "metadata": {
        "id": "_ZapWAM2_2y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Sanity check following filtering\n",
        "# cd_df_filtered.describe()\n",
        "# # or\n",
        "# cd_df_filtered['vehicle_id'].value_counts()\n"
      ],
      "metadata": {
        "id": "dCEz9G_PAokN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pickle replacement\n",
        "df_new = cd_df_filtered\n",
        "\n",
        "#Old code:\n",
        "#âœ… Step 1a: Load and Inspect the New Snapshot\n",
        "# new_snapshot_path = \"/content/cabq_gtfs_snapshots_20250722_1415.csv\"\n",
        "# df_new = pd.read_csv(new_snapshot_path)\n",
        "# df_new.info()\n",
        "# df_new.head(3)\n",
        "\n",
        "# #Also print the columns:\n",
        "# print(df_new.columns.tolist())\n"
      ],
      "metadata": {
        "id": "0ypQFlcHLSlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 2a: Trip ID Validity\n",
        "# Counts and displays the total rows and number of rows with invalid trip ids.\n",
        "# Calculates and displays the percentage of rows with valid trip ids.\n",
        "df_new[\"trip_id\"] = df_new[\"trip_id\"].astype(str)\n",
        "invalid_trip_ids = df_new[\"trip_id\"].isin([\"0\", \"Undetermined\", \"nan\", \"\", \"None\"]).sum()\n",
        "total_rows = len(df_new)\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"Invalid trip_ids: {invalid_trip_ids}\")\n",
        "print(f\"Percent valid trip_ids: {100 * (total_rows - invalid_trip_ids) / total_rows:.2f}%\")\n"
      ],
      "metadata": {
        "id": "4fmA5kemLoOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#*.v8.2 new calc for finding mostly Off Duty\n",
        "invalid_route_ids = df_new[\"route_short_name\"].isin([\"0\", \"Undetermined\", \"nan\", \"\", \"None\", \"Off Duty\"]).sum()\n",
        "print(f\"Invalid route_ids: {invalid_route_ids}\")\n",
        "print(f\"Percent valid route_ids: {100 * (total_rows - invalid_route_ids) / total_rows:.2f}%\")"
      ],
      "metadata": {
        "id": "_zTU-UVyiqWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_ids = [\"0\", \"Undetermined\", \"nan\", \"\", \"None\", \"Off Duty\"]\n",
        "df_clean_test = df_new[~df_new[\"trip_id\"].isin(invalid_ids) & ~df_new[\"route_short_name\"].isin(invalid_ids)].copy()\n",
        "df_clean_test.shape"
      ],
      "metadata": {
        "id": "71ssRDzZx3Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 2b: Filter invalid trip_ids\n",
        "invalid_trip_ids = [\"0\", \"Undetermined\", \"nan\", \"\", \"None\"]\n",
        "df_clean_trips = df_new[~df_new[\"trip_id\"].isin(invalid_trip_ids)].copy()\n",
        "df_clean_trips.shape"
      ],
      "metadata": {
        "id": "hRsg9R1sNJk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So after all that we can see that all of the rows dropped by df_clean_trips cover those dropped by df_clean_test (both invalid trips and routes)\n",
        "invalid_route_ids = df_clean_trips[\"route_short_name\"].isin([\"0\", \"Undetermined\", \"nan\", \"\", \"None\", \"Off Duty\"]).sum()\n",
        "print(f\"Invalid route_ids: {invalid_route_ids}\")\n",
        "print(f\"Percent valid route_ids: {100 * (total_rows - invalid_route_ids) / total_rows:.2f}%\")"
      ],
      "metadata": {
        "id": "MQpV0M5yzqld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean_test"
      ],
      "metadata": {
        "id": "1isiE5k5z8e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_clean.head()"
      ],
      "metadata": {
        "id": "ai8qg0ztQ3Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cleanup note\n",
        "We can skip several of the above cells, as dropping the invalid trip_ids also drops the invalid route_short_names"
      ],
      "metadata": {
        "id": "z8y8eMDb0MMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Static data"
      ],
      "metadata": {
        "id": "rHcIyBIuQtWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 1b: Reload Static GTFS and Normalize It (ensure data types are consistent)\n",
        "\n",
        "# Adjust if needed â€” make sure this is the static feed aligned with 2025-07-22\n",
        "gtfs_zip_path = \"/content/google_transit.zip\"\n",
        "\n",
        "with zipfile.ZipFile(gtfs_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/gtfs_static\")\n",
        "\n",
        "trips = pd.read_csv(\"/content/gtfs_static/trips.txt\", dtype=str)\n",
        "routes = pd.read_csv(\"/content/gtfs_static/routes.txt\", dtype=str)\n"
      ],
      "metadata": {
        "id": "_bLK0Q2PNDG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trips.shape"
      ],
      "metadata": {
        "id": "PzSbsj-v1Cig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trips.head()"
      ],
      "metadata": {
        "id": "SHJCH2Q02C0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trips.describe()"
      ],
      "metadata": {
        "id": "TxC-vRwF2UXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.shape"
      ],
      "metadata": {
        "id": "KbDnen351-mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.head()"
      ],
      "metadata": {
        "id": "zrKCkCnZ2JFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.describe(include='all')"
      ],
      "metadata": {
        "id": "1x1hjAq72cdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 3b: Merge with trips.txt to Get route_id\n",
        "# Take all of df_clean and add on data from trips if the trip_id matches across both dataframes.\n",
        "df_with_trips = df_clean.merge(trips, on=\"trip_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "QeT3MyKbNNs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_with_trips.head()"
      ],
      "metadata": {
        "id": "cgHqeRVlQ_Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_with_trips.columns"
      ],
      "metadata": {
        "id": "vp_E9RMERfev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Merge check\n",
        "1. Trip-Based Merges (Correct in Isolation)\n",
        "- df_with_trips = df_clean.merge(trips, on=\"trip_id\", how=\"left\")\n",
        "- df_full = df_with_trips.merge(routes, on=\"route_id\", how=\"left\")\n",
        "\n",
        "âœ… Good practice: merges are on appropriate keys (trip_id, route_id), and types are cast explicitly."
      ],
      "metadata": {
        "id": "L5Bj0dXS6S1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trips.columns"
      ],
      "metadata": {
        "id": "dhoxNHX3Rp4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.columns"
      ],
      "metadata": {
        "id": "RyNZZlZVRraT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.head()"
      ],
      "metadata": {
        "id": "cqU7VPk_R3-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell caused some issues in the merge because the df_with_trips and routes dfs did not agree on route_short_name, so we are going to take another approach\n",
        "# #âœ… Step 4b: Merge with routes.txt to Get Descriptive Info\n",
        "# df_with_trips[\"route_id\"] = df_with_trips[\"route_id\"].astype(str)\n",
        "\n",
        "# # Ensure consistent types\n",
        "# trips[\"shape_id\"] = trips[\"shape_id\"].astype(str)\n",
        "# routes[\"route_id\"] = routes[\"route_id\"].astype(str)\n",
        "\n",
        "# # Take all of df_with_trips and add on data from routes if the route_id matches across both dataframes.\n",
        "# df_full = df_with_trips.merge(routes, on=\"route_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "ax8zNn88NSNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's avoid the route_..._x and route_..._y situation on the merge:\n",
        "#âœ… Step 4b: Merge with routes.txt to Get Descriptive Info\n",
        "df_with_trips[\"route_id\"] = df_with_trips[\"route_id\"].astype(str)\n",
        "\n",
        "# Ensure consistent types\n",
        "trips[\"shape_id\"] = trips[\"shape_id\"].astype(str)\n",
        "routes[\"route_id\"] = routes[\"route_id\"].astype(str)\n",
        "\n",
        "# Assume 'df_with_trips' and 'routes' both contain 'route_id' and 'route_short_name'\n",
        "\n",
        "# Step 1: Check 1:1 match on overlapping column\n",
        "merged_check = df_with_trips[['route_id', 'route_short_name']].merge(\n",
        "    routes[['route_id', 'route_short_name']],\n",
        "    on='route_id',\n",
        "    how='inner',\n",
        "    suffixes=('_df_with_trips', '_routes')\n",
        ")\n"
      ],
      "metadata": {
        "id": "8oMjtdrxR-Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_check.head()"
      ],
      "metadata": {
        "id": "eBiPJKAPYjIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_check.info()"
      ],
      "metadata": {
        "id": "DMz85Q6xYs7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Find any mismatches\n",
        "mismatch = merged_check[merged_check['route_short_name_df_with_trips'] != merged_check['route_short_name_routes']]\n",
        "\n",
        "if not mismatch.empty:\n",
        "    raise ValueError(f\"Mismatch found in route_short_name across dataframes:\\n{mismatch}\")"
      ],
      "metadata": {
        "id": "R0uyAXt7Yefx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay... so there are some mismatches between the data and the static routes info; let's see what they are\n",
        "mismatch_summary = (\n",
        "    mismatch[['route_id', 'route_short_name_df_with_trips', 'route_short_name_routes']]\n",
        "    .drop_duplicates()\n",
        "    .sort_values('route_id')\n",
        ")\n",
        "display(mismatch_summary)\n"
      ],
      "metadata": {
        "id": "sxD_8Q8OZkPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This is the stopping point for 250731-1200; pick up here"
      ],
      "metadata": {
        "id": "u9WkbG17o6U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [k for k, v in globals().items() if str(type(v)) == \"<class 'pandas.core.frame.DataFrame'>\"]\n"
      ],
      "metadata": {
        "id": "5gP47R6G4ZFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.head()\n"
      ],
      "metadata": {
        "id": "dQ8R2shU4sED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.columns"
      ],
      "metadata": {
        "id": "U89_tc1n9pyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename to preserve both versions of route_short_name\n",
        "routes_renamed = routes.rename(columns={'route_short_name': 'route_short_name_static'})\n",
        "\n",
        "# Merge with df_with_trips, keeping the RT version as primary\n",
        "merged = df_with_trips.merge(routes_renamed, on='route_id', how='left')\n"
      ],
      "metadata": {
        "id": "LAcDQAzc42J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merged.columns"
      ],
      "metadata": {
        "id": "MzMvTV955TV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full = merged"
      ],
      "metadata": {
        "id": "HzpPdBlU9D5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_full.describe(include='all')"
      ],
      "metadata": {
        "id": "j_oPMi7G25L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #âœ… Step 5b: Load shapes.txt\n",
        "# shapes = pd.read_csv(\"/content/gtfs_static/shapes.txt\", dtype={\"shape_id\": str})"
      ],
      "metadata": {
        "id": "ovIo4n6t44ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shapes.describe(include='all')"
      ],
      "metadata": {
        "id": "c0NgX-n-46FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shapes.head(1000)"
      ],
      "metadata": {
        "id": "-vcRy7-05G0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 5b: Load shapes.txt\n",
        "shapes = pd.read_csv(\"/content/gtfs_static/shapes.txt\", dtype={\"shape_id\": str})\n",
        "\n",
        "# Build LineStrings for each shape_id\n",
        "shape_lines = {}\n",
        "for shape_id, group in shapes.groupby(\"shape_id\"):\n",
        "    sorted_group = group.sort_values(\"shape_pt_sequence\")\n",
        "    coords = list(zip(sorted_group[\"shape_pt_lon\"], sorted_group[\"shape_pt_lat\"]))\n",
        "    shape_lines[shape_id] = LineString(coords)\n",
        "\n",
        "# Merge trips and routes to link shape_id to route_short_name\n",
        "shape_route_map = (\n",
        "    trips.merge(routes, on=\"route_id\", how=\"left\")\n",
        "         .dropna(subset=[\"route_short_name\"])\n",
        "         .drop_duplicates(subset=[\"shape_id\"])\n",
        "         .set_index(\"shape_id\")[[\"route_id\", \"route_short_name\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "FPmkslyjUqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 6b: Create a data_quality Flag\n",
        "def classify_row(row):\n",
        "    if row[\"trip_id\"] in invalid_trip_ids:\n",
        "        return \"invalid_trip_id\"\n",
        "    elif pd.isna(row[\"route_id\"]):\n",
        "        return \"missing_route_id\"\n",
        "    elif pd.isna(row[\"route_long_name\"]):\n",
        "        return \"missing_route_metadata\"\n",
        "    else:\n",
        "        return \"valid\"\n",
        "\n",
        "df_full[\"data_quality\"] = df_full.apply(classify_row, axis=1)\n",
        "print(df_full[\"data_quality\"].value_counts())\n"
      ],
      "metadata": {
        "id": "KkvElwvqNWAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Possible Deprication"
      ],
      "metadata": {
        "id": "PVbAutuC8G7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Not sure if this is needed\n",
        "# #Load the trips, routes, shapes from Static data\n",
        "# with zipfile.ZipFile(gtfs_zip_path, 'r') as z:\n",
        "#     trips_df = pd.read_csv(z.open(\"trips.txt\"))\n",
        "#     routes_df = pd.read_csv(z.open(\"routes.txt\"))\n",
        "#     # Load GTFS shapes.txt into a DataFrame\n",
        "#     shapes_df = pd.read_csv(z.open(\"shapes.txt\"))"
      ],
      "metadata": {
        "id": "mobuNn50Xn93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean the RT data"
      ],
      "metadata": {
        "id": "1PdBamqvTD_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 1.1 Filter for Valid Rows\n",
        "df_valid = df_full[df_full[\"data_quality\"] == \"valid\"].copy()\n",
        "\n",
        "#ðŸ”¹ 1.2 Parse Timestamps\n",
        "df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "\n",
        "#ðŸ”¹ 1.3 Sort by Vehicle and Timestamp\n",
        "df_valid = df_valid.sort_values(by=[\"vehicle_id\", \"timestamp\"])\n",
        "\n",
        "#ðŸ”¹ 1.4 Organize by Vehicle\n",
        "#This creates a dictionary keyed by vehicle ID, each with a sorted DataFrame:\n",
        "vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "#You can confirm how many distinct vehicles youâ€™re tracking:\n",
        "print(\"Vehicle count:\", len(vehicle_groups))\n"
      ],
      "metadata": {
        "id": "L0NjFMGZTHp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_full.shape"
      ],
      "metadata": {
        "id": "lXJPO9u18dxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_full.describe(include='all')"
      ],
      "metadata": {
        "id": "QR91XWeV8n8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_full.head()"
      ],
      "metadata": {
        "id": "LA-WEw129qhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_valid.shape"
      ],
      "metadata": {
        "id": "fzdkigRc8aOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_valid.describe(include='all')"
      ],
      "metadata": {
        "id": "Klmg8mI_8rlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_valid.head(1000)"
      ],
      "metadata": {
        "id": "Y4KoiAkS85Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Anomly Detection"
      ],
      "metadata": {
        "id": "QVXWXP9FRX1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ 1. Detect Jumps and Gaps\n",
        "\n",
        "Already implemented, but hereâ€™s the modular form:"
      ],
      "metadata": {
        "id": "EaP4VCejSudu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 1. Detect Jumps and Gaps\n",
        "# def detect_jumps_and_gaps(df, params):\n",
        "#     \"\"\"\n",
        "#     Detects jump or gap anomalies based on distance (meters) and time (seconds)\n",
        "#     thresholds pulled from the anomaly_params dictionary.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Filtered DataFrame for a single vehicle\n",
        "#         params (dict): Anomaly detection thresholds from widget interface\n",
        "\n",
        "#     Returns:\n",
        "#         List[dict]: List of detected jump or gap anomalies\n",
        "#     \"\"\"\n",
        "#     distance_threshold = params[\"JUMP_DISTANCE_METERS\"]           # in meters\n",
        "#     time_threshold = params[\"DISAPPEARANCE_TIME_THRESHOLD\"]       # in seconds\n",
        "\n",
        "#     anomalies = []\n",
        "#     for i in range(1, len(df)):\n",
        "#         row_prev, row_curr = df.iloc[i - 1], df.iloc[i]\n",
        "#         time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "\n",
        "#         distance = geodesic(\n",
        "#             (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "#             (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "#         ).meters\n",
        "\n",
        "#         if time_diff > time_threshold or distance > distance_threshold:\n",
        "#             anomalies.append({\n",
        "#                 \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "#                 \"timestamp_prev\": row_prev[\"timestamp\"],\n",
        "#                 \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "#                 \"time_diff_sec\": time_diff,\n",
        "#                 \"distance_m\": distance,\n",
        "#                 \"is_gap\": time_diff > time_threshold,\n",
        "#                 \"is_jump\": distance > distance_threshold,\n",
        "#                 \"anomaly_type\": \"jump_or_gap\"\n",
        "#             })\n",
        "\n",
        "#     return anomalies\n",
        "# #Note: changed row[\"timestamp_collected\"] to row[\"timestamp\"] throughout, assuming that we've already converted timestamp_collected to UTC and assigned it to a new \"timestamp\" column at the start of the anomaly pipeline (which the existing pipeline does). This avoids inconsistency."
      ],
      "metadata": {
        "id": "vLHWx6tPmOXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 1. Detect Jumps and Gaps â€“ WITHIN trip_id\n",
        "def detect_jumps_and_gaps(df, params):\n",
        "    \"\"\"\n",
        "     Detects jump or gap anomalies based on distance (meters) and time (seconds)\n",
        "     thresholds pulled from the anomaly_params dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Filtered DataFrame for a single vehicle, sorted by timestamp.\n",
        "                           Must include 'trip_id', 'timestamp', 'latitude', 'longitude'.\n",
        "        params (dict): Anomaly detection thresholds from widget interface.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: Detected jump or gap anomalies.\n",
        "    \"\"\"\n",
        "    distance_threshold = params[\"JUMP_DISTANCE_METERS\"]           # meters\n",
        "    time_threshold = params[\"DISAPPEARANCE_TIME_THRESHOLD\"]       # seconds\n",
        "\n",
        "    anomalies = []\n",
        "    for i in range(1, len(df)):\n",
        "        row_prev, row_curr = df.iloc[i - 1], df.iloc[i]\n",
        "\n",
        "        # ðŸ’¡ Trip boundary check â€” skip if different trip\n",
        "        if row_prev[\"trip_id\"] != row_curr[\"trip_id\"]:\n",
        "            continue\n",
        "\n",
        "        time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "\n",
        "        distance = geodesic(\n",
        "            (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "            (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "        ).meters\n",
        "\n",
        "        if time_diff > time_threshold or distance > distance_threshold:\n",
        "            anomalies.append({\n",
        "                \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "                \"trip_id\": row_curr[\"trip_id\"],  # optional but useful for tracing\n",
        "                \"timestamp_prev\": row_prev[\"timestamp\"],\n",
        "                \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "                \"time_diff_sec\": time_diff,\n",
        "                \"distance_m\": distance,\n",
        "                \"is_gap\": time_diff > time_threshold,\n",
        "                \"is_jump\": distance > distance_threshold,\n",
        "                \"anomaly_type\": \"jump_or_gap\"\n",
        "            })\n",
        "\n",
        "    return anomalies\n"
      ],
      "metadata": {
        "id": "dBbOnb8YCzFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #2. Detect Stuck Vehicles\n",
        "# def detect_stuck_vehicle(df, params):\n",
        "#     \"\"\"\n",
        "#     Detects stuck vehicles using a rolling window of speed and spatial consistency,\n",
        "#     segmented by trip_id to avoid cross-trip contamination.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Must contain 'speed_mph', 'latitude', 'longitude', and 'trip_id'.\n",
        "#         params (dict): Widget-defined anomaly thresholds.\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Rows flagged as 'stuck_vehicle'.\n",
        "#     \"\"\"\n",
        "#     speed_thresh = params.get(\"STUCK_SPEED_MPH\", 1.0)   # Default: 1 mph\n",
        "#     window = params.get(\"STUCK_WINDOW_SIZE\", 4)         # Default: 4-frame window\n",
        "\n",
        "#     stuck_flags = (\n",
        "#         (df[\"speed_mph\"].rolling(window).mean() < speed_thresh) &\n",
        "#         (df[\"latitude\"].diff().abs().rolling(window).mean() < 0.0001) &\n",
        "#         (df[\"longitude\"].diff().abs().rolling(window).mean() < 0.0001)\n",
        "#     )\n",
        "\n",
        "#     return df[stuck_flags.fillna(False)].assign(anomaly_type=\"stuck_vehicle\")\n"
      ],
      "metadata": {
        "id": "5FTC1YHheFou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 2. Detect Stuck Vehicles\n",
        "def detect_stuck_vehicle(df, params):\n",
        "    \"\"\"\n",
        "    Detects stuck vehicles using a rolling window of speed and spatial consistency,\n",
        "    segmented by trip_id to avoid cross-trip contamination.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Must contain 'speed_mph', 'latitude', 'longitude', and 'trip_id'.\n",
        "        params (dict): Widget-defined anomaly thresholds.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Flagged stuck vehicle anomalies.\n",
        "    \"\"\"\n",
        "    speed_thresh = params.get(\"STUCK_SPEED_MPH\", 1.0)   # mph\n",
        "    window = params.get(\"STUCK_WINDOW_SIZE\", 4)         # frames\n",
        "\n",
        "    flagged_list = []\n",
        "\n",
        "    for trip_id, group in df.groupby(\"trip_id\"):\n",
        "        group = group.sort_values(\"timestamp\").copy()\n",
        "\n",
        "        # Compute rolling conditions\n",
        "        speed_ok = group[\"speed_mph\"].rolling(window).mean() < speed_thresh\n",
        "        lat_ok = group[\"latitude\"].diff().abs().rolling(window).mean() < 0.0001\n",
        "        lon_ok = group[\"longitude\"].diff().abs().rolling(window).mean() < 0.0001\n",
        "\n",
        "        stuck_flags = speed_ok & lat_ok & lon_ok\n",
        "\n",
        "        flagged = group[stuck_flags.fillna(False)].copy()\n",
        "        if not flagged.empty:\n",
        "            flagged[\"anomaly_type\"] = \"stuck_vehicle\"\n",
        "            flagged_list.append(flagged)\n",
        "\n",
        "    return pd.concat(flagged_list, ignore_index=True) if flagged_list else pd.DataFrame(columns=df.columns.tolist() + [\"anomaly_type\"])\n"
      ],
      "metadata": {
        "id": "wQLuVlBJEvsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 3. Detect Impossible Speeds\n",
        "# def detect_impossible_speeds(df, params):\n",
        "#     \"\"\"\n",
        "#     Detects movement segments where computed speed exceeds threshold.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Filtered or per-vehicle data\n",
        "#         params (dict): Anomaly detection parameters from UI\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Rows with flagged impossible-speed anomalies\n",
        "#     \"\"\"\n",
        "#     # Convert mph to kph for geodesic-based calc\n",
        "#     speed_limit_kph = params[\"SPEED_LIMIT_MPH\"] * 1.60934\n",
        "\n",
        "#     records = []\n",
        "#     for i in range(1, len(df)):\n",
        "#         row_prev, row_curr = df.iloc[i - 1], df.iloc[i]\n",
        "#         time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "#         if time_diff == 0:\n",
        "#             continue\n",
        "\n",
        "#         distance = geodesic(\n",
        "#             (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "#             (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "#         ).meters\n",
        "\n",
        "#         speed_kph = (distance / time_diff) * 3.6  # m/s â†’ km/h\n",
        "\n",
        "#         if speed_kph > speed_limit_kph:\n",
        "#             records.append({\n",
        "#                 \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "#                 \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "#                 \"computed_speed_kph\": speed_kph,\n",
        "#                 \"distance_m\": distance,\n",
        "#                 \"anomaly_type\": \"impossible_speed\"\n",
        "#             })\n",
        "\n",
        "#     return pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "ew0p0WY_oGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 3. Detect Impossible Speeds\n",
        "def detect_impossible_speeds(df, params):\n",
        "    \"\"\"\n",
        "    Detects movement segments where computed speed exceeds threshold,\n",
        "    segmented by trip_id to avoid cross-trip errors.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Must contain timestamp, lat/lon, trip_id\n",
        "        params (dict): Anomaly detection thresholds\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Flagged rows with 'impossible_speed'\n",
        "    \"\"\"\n",
        "    speed_limit_kph = params[\"SPEED_LIMIT_MPH\"] * 1.60934\n",
        "    output_rows = []\n",
        "\n",
        "    for trip_id, group in df.groupby(\"trip_id\"):\n",
        "        group = group.sort_values(\"timestamp\").copy()\n",
        "\n",
        "        for i in range(1, len(group)):\n",
        "            row_prev, row_curr = group.iloc[i - 1], group.iloc[i]\n",
        "            time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "            if time_diff <= 0:\n",
        "                continue\n",
        "\n",
        "            distance = geodesic(\n",
        "                (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "                (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "            ).meters\n",
        "\n",
        "            speed_kph = (distance / time_diff) * 3.6\n",
        "\n",
        "            if speed_kph > speed_limit_kph:\n",
        "                output_rows.append({\n",
        "                    \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "                    \"trip_id\": trip_id,\n",
        "                    \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "                    \"computed_speed_kph\": speed_kph,\n",
        "                    \"distance_m\": distance,\n",
        "                    \"anomaly_type\": \"impossible_speed\"\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(output_rows)\n"
      ],
      "metadata": {
        "id": "FOSv2R79H2e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 4. Detect Backtracking (Heading Reversal)\n",
        "# def detect_backtracking(df, params):\n",
        "#     \"\"\"\n",
        "#     Detects heading reversals suggesting backtracking behavior.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Filtered or per-vehicle data\n",
        "#         params (dict): Anomaly detection parameters from UI\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Rows where heading reversed beyond threshold\n",
        "#     \"\"\"\n",
        "#     reversal_thresh = params[\"REVERSAL_HEADING_THRESHOLD\"]\n",
        "\n",
        "#     if \"heading\" not in df.columns:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     heading_diff = df[\"heading\"].diff().abs()\n",
        "#     backtrack_flags = heading_diff.between(reversal_thresh, 200)\n",
        "\n",
        "#     return df[backtrack_flags.fillna(False)].assign(anomaly_type=\"backtracking\")\n"
      ],
      "metadata": {
        "id": "zn0yhTHEoRGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 4. Detect Backtracking (Heading Reversal)\n",
        "def detect_backtracking(df, params):\n",
        "    \"\"\"\n",
        "    Detects heading reversals (backtracking) within trip boundaries only,\n",
        "    with wraparound-safe angle diffing and traceability.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Filtered per-vehicle data with heading + trip_id\n",
        "        params (dict): Anomaly detection thresholds\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Rows flagged as 'backtracking'\n",
        "    \"\"\"\n",
        "    reversal_thresh = params.get(\"REVERSAL_HEADING_THRESHOLD\", 120)\n",
        "    output_frames = []\n",
        "\n",
        "    if \"heading\" not in df.columns or \"trip_id\" not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for trip_id, group in df.groupby(\"trip_id\"):\n",
        "        group = group.sort_values(\"timestamp\").copy()\n",
        "\n",
        "        # Compute heading difference with wraparound correction\n",
        "        group[\"heading_prev\"] = group[\"heading\"].shift()\n",
        "        group[\"heading_diff\"] = (group[\"heading\"] - group[\"heading_prev\"]).abs()\n",
        "        group[\"heading_diff\"] = group[\"heading_diff\"].apply(\n",
        "            lambda x: min(x, 360 - x) if pd.notna(x) else x\n",
        "        )\n",
        "\n",
        "        # Identify backtracking\n",
        "        backtrack_flags = group[\"heading_diff\"].between(reversal_thresh, 200)\n",
        "\n",
        "        flagged = group[backtrack_flags.fillna(False)].copy()\n",
        "        flagged[\"anomaly_type\"] = \"backtracking\"\n",
        "        flagged[\"timestamp_prev\"] = group[\"timestamp\"].shift()\n",
        "\n",
        "        output_frames.append(flagged)\n",
        "\n",
        "    return pd.concat(output_frames, ignore_index=True) if output_frames else pd.DataFrame()\n",
        "\n"
      ],
      "metadata": {
        "id": "8fVw8M7YIiZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 5. Detect Repeated Points\n",
        "# def detect_repeated_points(df, params):\n",
        "#     \"\"\"\n",
        "#     Detects repeated GPS coordinates (vehicle not moving).\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Filtered or per-vehicle snapshot data\n",
        "#         params (dict): Anomaly detection parameters from UI\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Rows flagged as having repeated lat/lon values\n",
        "#     \"\"\"\n",
        "#     tolerance = abs(params.get(\"REPEATED_COORD_TOLERANCE\", 1e-5))\n",
        "\n",
        "#     if \"latitude\" not in df.columns or \"longitude\" not in df.columns:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     repeated = (\n",
        "#         (df[\"latitude\"].diff().abs() < tolerance) &\n",
        "#         (df[\"longitude\"].diff().abs() < tolerance)\n",
        "#     )\n",
        "\n",
        "#     return df[repeated.fillna(False)].assign(anomaly_type=\"repeated_points\")\n"
      ],
      "metadata": {
        "id": "jGu4oemRoWkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 5. Detect Repeated Points\n",
        "def detect_repeated_points(df, params):\n",
        "    \"\"\"\n",
        "    Detects repeated GPS coordinates (vehicle not moving), scoped to each trip.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Per-vehicle GPS snapshot data\n",
        "        params (dict): UI-provided anomaly detection thresholds\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Rows flagged as 'repeated_points'\n",
        "    \"\"\"\n",
        "    tolerance = abs(params.get(\"REPEATED_COORD_TOLERANCE\", 1e-5))\n",
        "    output = []\n",
        "\n",
        "    if \"latitude\" not in df.columns or \"longitude\" not in df.columns or \"trip_id\" not in df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for trip_id, group in df.groupby(\"trip_id\"):\n",
        "        group = group.sort_values(\"timestamp\").copy()\n",
        "        lat_repeat = group[\"latitude\"].diff().abs() < tolerance\n",
        "        lon_repeat = group[\"longitude\"].diff().abs() < tolerance\n",
        "        repeated = lat_repeat & lon_repeat\n",
        "\n",
        "        flagged = group[repeated.fillna(False)].copy()\n",
        "        flagged[\"anomaly_type\"] = \"repeated_points\"\n",
        "        output.append(flagged)\n",
        "\n",
        "    return pd.concat(output, ignore_index=True) if output else pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "c2j_294WIj5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 6. Detect Disappearance Without Return\n",
        "# def detect_disappeared(df, snapshot_end_time, params):\n",
        "#     \"\"\"\n",
        "#     Detects vehicles that have not reappeared by the end of the snapshot period.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (pd.DataFrame): Data for a single vehicle\n",
        "#         snapshot_end_time (datetime): End of data collection\n",
        "#         params (dict): Anomaly detection thresholds from widget\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Single-row disappearance anomaly, or empty DataFrame\n",
        "#     \"\"\"\n",
        "#     if df.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     last_seen = df[\"timestamp\"].max()\n",
        "#     time_gap_sec = (snapshot_end_time - last_seen).total_seconds()\n",
        "#     disappearance_thresh = params[\"DISAPPEARANCE_TIME_THRESHOLD\"]\n",
        "\n",
        "#     if time_gap_sec > disappearance_thresh:\n",
        "#         return pd.DataFrame([{\n",
        "#             \"vehicle_id\": df[\"vehicle_id\"].iloc[0],\n",
        "#             \"last_seen\": last_seen,\n",
        "#             \"anomaly_type\": \"disappearance\"\n",
        "#         }])\n",
        "\n",
        "#     return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "BHaxgOWLogKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 6. Detect Disappearance Without Return\n",
        "def detect_disappeared(df, snapshot_end_time, params):\n",
        "    \"\"\"\n",
        "    Detects disappearance per trip_id â€” if the trip ends and never resumes.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Data for a single vehicle\n",
        "        snapshot_end_time (datetime): End of snapshot\n",
        "        params (dict): Anomaly config\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: One row per disappeared trip, if any\n",
        "    \"\"\"\n",
        "    disappearance_thresh = params[\"DISAPPEARANCE_TIME_THRESHOLD\"]\n",
        "    output = []\n",
        "\n",
        "    for trip_id, group in df.groupby(\"trip_id\"):\n",
        "        last_seen = group[\"timestamp\"].max()\n",
        "        time_gap_sec = (snapshot_end_time - last_seen).total_seconds()\n",
        "\n",
        "        if time_gap_sec > disappearance_thresh:\n",
        "            output.append({\n",
        "                \"vehicle_id\": group[\"vehicle_id\"].iloc[0],\n",
        "                \"trip_id\": trip_id,\n",
        "                \"last_seen\": last_seen,\n",
        "                \"anomaly_type\": \"disappearance\"\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(output) if output else pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "YIp7s69FIvEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 7. Detect Early Appearance\n",
        "def detect_early_appearance(df, snapshot_start_time, params):\n",
        "    \"\"\"\n",
        "    Detects vehicles that appear too early (likely pre-start ghost data).\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Data for a single vehicle\n",
        "        snapshot_start_time (datetime): Start of capture window\n",
        "        params (dict): Anomaly detection thresholds from widget\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Single-row anomaly, or empty DataFrame\n",
        "    \"\"\"\n",
        "    margin_seconds = params.get(\"EARLY_APPEARANCE_MARGIN_SEC\", 30)\n",
        "    first_seen = df[\"timestamp\"].min()\n",
        "    delta = (first_seen - snapshot_start_time).total_seconds()\n",
        "\n",
        "    if delta < margin_seconds:\n",
        "        return pd.DataFrame([{\n",
        "            \"vehicle_id\": df[\"vehicle_id\"].iloc[0],\n",
        "            \"first_seen\": first_seen,\n",
        "            \"anomaly_type\": \"early_appearance\"\n",
        "        }])\n",
        "    return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "ZahOYYt1onvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ðŸ”¹ 8. Detect Off-Route Movement\n",
        "# def detect_off_route(df_vehicle, shape_lines, params):\n",
        "#     \"\"\"\n",
        "#     Flags GPS points that are farther than OFF_ROUTE_BUFFER_M from the expected route shape.\n",
        "\n",
        "#     Parameters:\n",
        "#         df_vehicle (pd.DataFrame): All points for a single vehicle\n",
        "#         shape_lines (dict): Dictionary of LineStrings per shape_id\n",
        "#         params (dict): Thresholds including OFF_ROUTE_BUFFER_M\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Off-route GPS points\n",
        "#     \"\"\"\n",
        "#     buffer_m = params.get(\"OFF_ROUTE_BUFFER_M\", 50)\n",
        "#     records = []\n",
        "\n",
        "#     for _, row in df_vehicle.iterrows():\n",
        "#         shape_id = str(row.get(\"shape_id\"))\n",
        "#         if shape_id not in shape_lines:\n",
        "#             continue  # shape not available\n",
        "\n",
        "#         route_line = shape_lines[shape_id]\n",
        "#         vehicle_point = Point(row[\"longitude\"], row[\"latitude\"])\n",
        "\n",
        "#         # Project point onto shape line and compute geodesic distance\n",
        "#         closest_point = route_line.interpolate(route_line.project(vehicle_point))\n",
        "#         dist_m = geodesic(\n",
        "#             (row[\"latitude\"], row[\"longitude\"]),\n",
        "#             (closest_point.y, closest_point.x)\n",
        "#         ).meters\n",
        "\n",
        "#         if dist_m > buffer_m:\n",
        "#             records.append({\n",
        "#                 \"vehicle_id\": row[\"vehicle_id\"],\n",
        "#                 \"timestamp\": row[\"timestamp\"],\n",
        "#                 \"route_short_name\": row.get(\"route_short_name\"),\n",
        "#                 \"distance_from_route_m\": dist_m,\n",
        "#                 \"latitude\": row[\"latitude\"],\n",
        "#                 \"longitude\": row[\"longitude\"],\n",
        "#                 \"shape_id\": shape_id,\n",
        "#                 \"anomaly_type\": \"off_route\"\n",
        "#             })\n",
        "\n",
        "#     return pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "ayfGmn49os0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ 8. Detect Off-Route Movement\n",
        "def detect_off_route(df_vehicle, shape_lines, params):\n",
        "    \"\"\"\n",
        "    Detect off-route anomalies using trip-level shape validation.\n",
        "\n",
        "    Parameters:\n",
        "        df_vehicle (pd.DataFrame): All rows for a single vehicle.\n",
        "        shape_lines (dict): shape_id â†’ LineString\n",
        "        params (dict): Dictionary with OFF_ROUTE_BUFFER_M\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Flagged off-route points\n",
        "    \"\"\"\n",
        "    buffer_m = params.get(\"OFF_ROUTE_BUFFER_M\", 50)\n",
        "    records = []\n",
        "\n",
        "    for trip_id, trip_df in df_vehicle.groupby(\"trip_id\"):\n",
        "        # Check if all rows share the same shape_id\n",
        "        shape_ids = trip_df[\"shape_id\"].dropna().unique()\n",
        "\n",
        "        if len(shape_ids) != 1:\n",
        "            # Ambiguous shape_id â€” skip this trip to avoid misflagging\n",
        "            continue\n",
        "\n",
        "        shape_id = str(shape_ids[0])\n",
        "        if shape_id not in shape_lines:\n",
        "            continue\n",
        "\n",
        "        route_line = shape_lines[shape_id]\n",
        "\n",
        "        for _, row in trip_df.iterrows():\n",
        "            vehicle_point = Point(row[\"longitude\"], row[\"latitude\"])\n",
        "            projected = route_line.interpolate(route_line.project(vehicle_point))\n",
        "            dist_m = geodesic(\n",
        "                (row[\"latitude\"], row[\"longitude\"]),\n",
        "                (projected.y, projected.x)\n",
        "            ).meters\n",
        "\n",
        "            if dist_m > buffer_m:\n",
        "                records.append({\n",
        "                    \"vehicle_id\": row[\"vehicle_id\"],\n",
        "                    \"trip_id\": trip_id,\n",
        "                    \"timestamp\": row[\"timestamp\"],\n",
        "                    \"route_short_name\": row.get(\"route_short_name\"),\n",
        "                    \"distance_from_route_m\": dist_m,\n",
        "                    \"latitude\": row[\"latitude\"],\n",
        "                    \"longitude\": row[\"longitude\"],\n",
        "                    \"shape_id\": shape_id,\n",
        "                    \"anomaly_type\": \"off_route\"\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Note: âœ… Key Protections Added\n",
        "# Fix\tExplanation\n",
        "# groupby(\"trip_id\")\tEnsures anomalies only detected within a single trip context\n",
        "# unique shape_id check\tFilters out data where shape mapping is ambiguous\n",
        "# skip trip if shape_id missing or mismatched\tPrevents false positives from misassigned shapes\n",
        "# Includes trip_id in output\tImproves downstream traceability and debugging"
      ],
      "metadata": {
        "id": "OGUZKOjxIyTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define options\n",
        "detector_options = [\n",
        "    (\"Jumps & Gaps\", \"jumpgap\"),\n",
        "    (\"Stuck Vehicles\", \"stuck\"),\n",
        "    (\"Impossible Speeds\", \"speed\"),\n",
        "    (\"Backtracking\", \"backtrack\"),\n",
        "    (\"Repeated Points\", \"repeated\"),\n",
        "    (\"Disappearance\", \"disappear\"),\n",
        "    (\"Early Appearance\", \"early\"),\n",
        "    (\"Off-Route\", \"offroute\")\n",
        "]\n",
        "\n",
        "# Create selector widget\n",
        "detector_selector = widgets.SelectMultiple(\n",
        "    options=detector_options,\n",
        "    value=[k for _, k in detector_options],  # default: all selected\n",
        "    description=\"Detectors:\",\n",
        "    layout=widgets.Layout(width=\"60%\")\n",
        ")\n",
        "\n",
        "# Create button\n",
        "confirm_button = widgets.Button(\n",
        "    description=\"Confirm Selection\",\n",
        "    button_style='primary',\n",
        "    icon='check'\n",
        ")\n",
        "\n",
        "# Output area\n",
        "selection_output = widgets.Output()\n",
        "\n",
        "# Define callback\n",
        "def on_confirm_clicked(b):\n",
        "    with selection_output:\n",
        "        clear_output()\n",
        "        selected = list(detector_selector.value)\n",
        "        print(\"âœ… Selected anomaly detectors:\")\n",
        "        for detector in selected:\n",
        "            print(f\" - {detector}\")\n",
        "        # Optionally set a global variable for later use:\n",
        "        global selected_detectors\n",
        "        selected_detectors = selected\n",
        "\n",
        "# Link button to callback\n",
        "confirm_button.on_click(on_confirm_clicked)\n",
        "\n",
        "# Display widgets\n",
        "display(widgets.VBox([detector_selector, confirm_button, selection_output]))\n"
      ],
      "metadata": {
        "id": "aO71Y2oKUcJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # I believe we can skip step 1 because it was already done earlier\n",
        "# # # Step 1: Normalize timestamp and rebuild vehicle groups\n",
        "# # df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "# # vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "# # Step 2: Detect each anomaly type using widget-supplied parameters\n",
        "# jumpgap_records = []\n",
        "# stuck_records = []\n",
        "# speed_records = []\n",
        "# backtrack_records = []\n",
        "# repeated_records = []\n",
        "# disappear_records = []\n",
        "# early_records = []\n",
        "# offroute_records = []\n",
        "\n",
        "# snapshot_start = df_valid[\"timestamp\"].min()\n",
        "# snapshot_end = df_valid[\"timestamp\"].max()\n",
        "\n",
        "# for vehicle_id, df_vehicle in vehicle_groups.items():\n",
        "#     df_vehicle = df_vehicle.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "#     # 1. Jumps and Gaps\n",
        "#     jumpgap_records.extend(detect_jumps_and_gaps(df_vehicle, params=anomaly_params))\n",
        "\n",
        "#     # 2. Stuck Vehicles\n",
        "#     stuck = detect_stuck_vehicle(df_vehicle, params=anomaly_params)\n",
        "#     if not stuck.empty:\n",
        "#         stuck_records.append(stuck)\n",
        "\n",
        "#     # 3. Impossible Speeds\n",
        "#     speed = detect_impossible_speeds(df_vehicle, params=anomaly_params)\n",
        "#     if not speed.empty:\n",
        "#         speed_records.append(speed)\n",
        "\n",
        "#     # 4. Backtracking\n",
        "#     backtrack = detect_backtracking(df_vehicle, params=anomaly_params)\n",
        "#     if not backtrack.empty:\n",
        "#         backtrack_records.append(backtrack)\n",
        "\n",
        "#     # 5. Repeated Points\n",
        "#     repeat = detect_repeated_points(df_vehicle, params=anomaly_params)\n",
        "#     if not repeat.empty:\n",
        "#         repeated_records.append(repeat)\n",
        "\n",
        "#     # 6. Disappearance\n",
        "#     disappear = detect_disappeared(df_vehicle, snapshot_end_time=snapshot_end, params=anomaly_params)\n",
        "#     if not disappear.empty:\n",
        "#         disappear_records.append(disappear)\n",
        "\n",
        "#     # 7. Early Appearance\n",
        "#     early = detect_early_appearance(df_vehicle, snapshot_start_time=snapshot_start, params=anomaly_params)\n",
        "#     if not early.empty:\n",
        "#         early_records.append(early)\n",
        "\n",
        "#     # 8. Off-Route\n",
        "#     offroute = detect_off_route(df_vehicle, shape_lines=shape_lines, params=anomaly_params)\n",
        "#     if not offroute.empty:\n",
        "#         offroute_records.append(offroute)\n",
        "\n",
        "# # Step 3: Combine to DataFrames\n",
        "# df_anomalies_jumpgap   = pd.DataFrame(jumpgap_records)\n",
        "# df_anomalies_stuck     = pd.concat(stuck_records, ignore_index=True)     if stuck_records     else pd.DataFrame()\n",
        "# df_anomalies_speed     = pd.concat(speed_records, ignore_index=True)     if speed_records     else pd.DataFrame()\n",
        "# df_anomalies_backtrack = pd.concat(backtrack_records, ignore_index=True) if backtrack_records else pd.DataFrame()\n",
        "# df_anomalies_repeated  = pd.concat(repeated_records, ignore_index=True)  if repeated_records  else pd.DataFrame()\n",
        "# df_anomalies_disappear = pd.concat(disappear_records, ignore_index=True) if disappear_records else pd.DataFrame()\n",
        "# df_anomalies_early     = pd.concat(early_records, ignore_index=True)     if early_records     else pd.DataFrame()\n",
        "# df_anomalies_offroute  = pd.concat(offroute_records, ignore_index=True)  if offroute_records  else pd.DataFrame()\n",
        "\n",
        "# # Step 4: Combine all anomalies into a single DataFrame\n",
        "# anomaly_frames = [\n",
        "#     df_anomalies_jumpgap,\n",
        "#     df_anomalies_stuck,\n",
        "#     df_anomalies_speed,\n",
        "#     df_anomalies_backtrack,\n",
        "#     df_anomalies_repeated,\n",
        "#     df_anomalies_disappear,\n",
        "#     df_anomalies_early,\n",
        "#     df_anomalies_offroute\n",
        "# ]\n",
        "\n",
        "# anomaly_frames = [df for df in anomaly_frames if 'anomaly_type' in df.columns and not df.empty]\n",
        "# df_anomalies_full = pd.concat(anomaly_frames, ignore_index=True)\n",
        "\n",
        "# # Report\n",
        "# print(\"âœ… Unified anomaly count:\", len(df_anomalies_full))\n",
        "# display(df_anomalies_full[\"anomaly_type\"].value_counts())\n"
      ],
      "metadata": {
        "id": "W8zAPnNWo1cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected detectors from UI\n",
        "selected_detectors = list(detector_selector.value)\n",
        "\n",
        "# Reset result containers\n",
        "jumpgap_records = []\n",
        "stuck_records = []\n",
        "speed_records = []\n",
        "backtrack_records = []\n",
        "repeated_records = []\n",
        "disappear_records = []\n",
        "early_records = []\n",
        "offroute_records = []\n",
        "\n",
        "snapshot_start = df_valid[\"timestamp\"].min()\n",
        "snapshot_end = df_valid[\"timestamp\"].max()\n",
        "\n",
        "# Run detectors conditionally\n",
        "for vehicle_id, df_vehicle in vehicle_groups.items():\n",
        "    df_vehicle = df_vehicle.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "    if \"jumpgap\" in selected_detectors:\n",
        "        jumpgap_records.extend(detect_jumps_and_gaps(df_vehicle, params=anomaly_params))\n",
        "\n",
        "    if \"stuck\" in selected_detectors:\n",
        "        stuck = detect_stuck_vehicle(df_vehicle, params=anomaly_params)\n",
        "        if not stuck.empty:\n",
        "            stuck_records.append(stuck)\n",
        "\n",
        "    if \"speed\" in selected_detectors:\n",
        "        speed = detect_impossible_speeds(df_vehicle, params=anomaly_params)\n",
        "        if not speed.empty:\n",
        "            speed_records.append(speed)\n",
        "\n",
        "    if \"backtrack\" in selected_detectors:\n",
        "        backtrack = detect_backtracking(df_vehicle, params=anomaly_params)\n",
        "        if not backtrack.empty:\n",
        "            backtrack_records.append(backtrack)\n",
        "\n",
        "    if \"repeated\" in selected_detectors:\n",
        "        repeat = detect_repeated_points(df_vehicle, params=anomaly_params)\n",
        "        if not repeat.empty:\n",
        "            repeated_records.append(repeat)\n",
        "\n",
        "    if \"disappear\" in selected_detectors:\n",
        "        disappear = detect_disappeared(df_vehicle, snapshot_end_time=snapshot_end, params=anomaly_params)\n",
        "        if not disappear.empty:\n",
        "            disappear_records.append(disappear)\n",
        "\n",
        "    if \"early\" in selected_detectors:\n",
        "        early = detect_early_appearance(df_vehicle, snapshot_start_time=snapshot_start, params=anomaly_params)\n",
        "        if not early.empty:\n",
        "            early_records.append(early)\n",
        "\n",
        "    if \"offroute\" in selected_detectors:\n",
        "        offroute = detect_off_route(df_vehicle, shape_lines=shape_lines, params=anomaly_params)\n",
        "        if not offroute.empty:\n",
        "            offroute_records.append(offroute)\n",
        "\n",
        "# Step 3: Combine to DataFrames\n",
        "df_anomalies_jumpgap   = pd.DataFrame(jumpgap_records)\n",
        "df_anomalies_stuck     = pd.concat(stuck_records, ignore_index=True)     if stuck_records     else pd.DataFrame()\n",
        "df_anomalies_speed     = pd.concat(speed_records, ignore_index=True)     if speed_records     else pd.DataFrame()\n",
        "df_anomalies_backtrack = pd.concat(backtrack_records, ignore_index=True) if backtrack_records else pd.DataFrame()\n",
        "df_anomalies_repeated  = pd.concat(repeated_records, ignore_index=True)  if repeated_records  else pd.DataFrame()\n",
        "df_anomalies_disappear = pd.concat(disappear_records, ignore_index=True) if disappear_records else pd.DataFrame()\n",
        "df_anomalies_early     = pd.concat(early_records, ignore_index=True)     if early_records     else pd.DataFrame()\n",
        "df_anomalies_offroute  = pd.concat(offroute_records, ignore_index=True)  if offroute_records  else pd.DataFrame()\n",
        "\n",
        "# Step 4: Combine all selected anomalies\n",
        "anomaly_frames = []\n",
        "if \"jumpgap\" in selected_detectors:   anomaly_frames.append(df_anomalies_jumpgap)\n",
        "if \"stuck\" in selected_detectors:     anomaly_frames.append(df_anomalies_stuck)\n",
        "if \"speed\" in selected_detectors:     anomaly_frames.append(df_anomalies_speed)\n",
        "if \"backtrack\" in selected_detectors: anomaly_frames.append(df_anomalies_backtrack)\n",
        "if \"repeated\" in selected_detectors:  anomaly_frames.append(df_anomalies_repeated)\n",
        "if \"disappear\" in selected_detectors: anomaly_frames.append(df_anomalies_disappear)\n",
        "if \"early\" in selected_detectors:     anomaly_frames.append(df_anomalies_early)\n",
        "if \"offroute\" in selected_detectors:  anomaly_frames.append(df_anomalies_offroute)\n",
        "\n",
        "anomaly_frames = [df for df in anomaly_frames if 'anomaly_type' in df.columns and not df.empty]\n",
        "df_anomalies_full = pd.concat(anomaly_frames, ignore_index=True) if anomaly_frames else pd.DataFrame()\n",
        "\n",
        "# Report\n",
        "print(\"Anomaly count (pre-dedup):\", len(df_anomalies_full))\n",
        "df_anomalies_full.drop_duplicates(inplace=True)\n",
        "print(\"Anomaly count (post-dedup):\", len(df_anomalies_full))\n",
        "\n",
        "print(\"âœ… Unified anomaly count:\", len(df_anomalies_full))\n",
        "if not df_anomalies_full.empty:\n",
        "    display(df_anomalies_full[\"anomaly_type\"].value_counts())\n",
        "else:\n",
        "    print(\"âš ï¸ No anomalies detected.\")\n"
      ],
      "metadata": {
        "id": "ov5aI3kDNl16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full.describe(include='all')"
      ],
      "metadata": {
        "id": "ubehDxqqfh0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full.head(100)"
      ],
      "metadata": {
        "id": "r7q4A8Aff3A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Total rows:\", len(df_anomalies_full))\n",
        "# print(\"Unique rows:\", len(df_anomalies_full.drop_duplicates()))\n",
        "# print(\"Duplicate rows:\", df_anomalies_full.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "GpnUdyGYZjiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Merge check\n",
        "2. Shape-Based Merge into df_anomalies_offroute\n",
        "- df_anomalies_offroute = df_anomalies_offroute.merge(\n",
        "    shape_route_map, on=\"shape_id\", how=\"left\", suffixes=(\"\", \"_from_map\")\n",
        ")\n",
        "\n",
        "ðŸ” Key issue: This assumes that df_anomalies_offroute has an accurate shape_id per row.\n",
        "\n",
        "But this is not always valid, because:\n",
        "\n",
        "- Anomalies are grouped by vehicle_id and timestamp.\n",
        "\n",
        "- There's no explicit trip_id or shape_id preserved per anomaly.\n",
        "\n",
        "- If a single vehicle changes shapes mid-route, this could assign the wrong route."
      ],
      "metadata": {
        "id": "wx4XnkJJ9L-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_route_map.head()"
      ],
      "metadata": {
        "id": "wEtp0FQLDbJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape_route_map.info()"
      ],
      "metadata": {
        "id": "WK151TTDD7AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_offroute.info()"
      ],
      "metadata": {
        "id": "IbFrcvAlDqzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_offroute['shape_id'].value_counts()"
      ],
      "metadata": {
        "id": "TZNRe356EPri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure shape_id is a column (not index)\n",
        "shape_route_map = shape_route_map.reset_index()\n",
        "\n",
        "# Ensure both shape_id columns are string type before merge\n",
        "df_anomalies_offroute[\"shape_id\"] = df_anomalies_offroute[\"shape_id\"].astype(str)\n",
        "shape_route_map[\"shape_id\"] = shape_route_map[\"shape_id\"].astype(str)\n",
        "\n",
        "# Join to enrich anomalies with route info\n",
        "df_anomalies_offroute = (\n",
        "    df_anomalies_offroute\n",
        "    .merge(shape_route_map, on=\"shape_id\", how=\"left\", suffixes=(\"\", \"_from_map\"))\n",
        ")\n",
        "\n",
        "# # Patch missing route names\n",
        "# df_anomalies_offroute[\"route_short_name\"] = (\n",
        "#     df_anomalies_offroute[\"route_short_name\"]\n",
        "#     .fillna(df_anomalies_offroute[\"route_short_name_from_map\"])\n",
        "# )\n",
        "\n",
        "# # Clean up\n",
        "# df_anomalies_offroute = df_anomalies_offroute.drop(columns=[\"route_short_name_from_map\"])\n"
      ],
      "metadata": {
        "id": "iGuHH4mQblN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_offroute.describe(include='all')"
      ],
      "metadata": {
        "id": "oRgwHmvcFFUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_offroute.isna().sum()"
      ],
      "metadata": {
        "id": "IQmk7N1FFa5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity Check\n",
        "missing_routes = df_anomalies_offroute[\"route_short_name\"].isnull().sum()\n",
        "print(f\"Remaining anomalies with missing route_short_name: {missing_routes}\")\n",
        "#If this prints 0, youâ€™ve successfully patched all entries.\n",
        "\n",
        "if missing_routes > 0:\n",
        "    print(\"Sample of anomalies with missing route_short_name:\")\n",
        "    display(df_anomalies_offroute[df_anomalies_offroute[\"route_short_name\"].isnull()].head())\n"
      ],
      "metadata": {
        "id": "vEr-NnroWCUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full.isna().sum()"
      ],
      "metadata": {
        "id": "xE6-IWvxGGl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full.columns"
      ],
      "metadata": {
        "id": "tuxK_B78Gsfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Debugging note: I believe the cell below may be creating some problems"
      ],
      "metadata": {
        "id": "fOtlKFTeglu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_full['data_quality'].value_counts()"
      ],
      "metadata": {
        "id": "Ynzh3CxBICXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 1.1 Filter for Valid Rows\n",
        "df_valid = df_full[df_full[\"data_quality\"] == \"valid\"].copy()\n",
        "\n",
        "#ðŸ”¹ 1.2 Parse Timestamps\n",
        "df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "\n",
        "#ðŸ”¹ 1.3 Sort by Vehicle and Timestamp\n",
        "df_valid = df_valid.sort_values(by=[\"vehicle_id\", \"timestamp\"])\n",
        "\n",
        "#ðŸ”¹ 1.4 Organize by Vehicle\n",
        "#This creates a dictionary keyed by vehicle ID, each with a sorted DataFrame:\n",
        "vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "#You can confirm how many distinct vehicles youâ€™re tracking:\n",
        "print(\"Vehicle count:\", len(vehicle_groups))\n"
      ],
      "metadata": {
        "id": "rjtc3efkP9GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary Stats by Route/Anomaly"
      ],
      "metadata": {
        "id": "_cUElQKNWSAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Debugging check: possible problem here with route_lookup"
      ],
      "metadata": {
        "id": "zs_Sev9QcHK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_valid.head(20)"
      ],
      "metadata": {
        "id": "lA4oDHeFhkyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_valid.columns"
      ],
      "metadata": {
        "id": "76lAmaoahJns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routes.columns"
      ],
      "metadata": {
        "id": "wXbrqL3whbMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trips.columns"
      ],
      "metadata": {
        "id": "7GYEsQLfKbpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build trip-to-route lookup from GTFS static data\n",
        "trip_to_route_lookup = trips[[\"trip_id\", \"route_id\"]].merge(\n",
        "    routes[[\"route_id\", \"route_short_name\"]], on=\"route_id\", how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "id": "vZx3bu-LK2q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_to_route_lookup.columns"
      ],
      "metadata": {
        "id": "Cq3NRBjmK5EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_to_route_lookup['route_short_name'].value_counts()"
      ],
      "metadata": {
        "id": "BZWf3B7OLNgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_to_route_lookup['route_id'].value_counts()"
      ],
      "metadata": {
        "id": "heakHpb3Ok06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build trip-to-route lookup from GTFS static data\n",
        "trip_to_route_lookup = trips[[\"trip_id\", \"route_id\"]].merge(\n",
        "    routes[[\"route_id\", \"route_short_name\"]], on=\"route_id\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Ensure trip_id types match\n",
        "df_valid[\"trip_id\"] = df_valid[\"trip_id\"].astype(str)\n",
        "trip_to_route_lookup[\"trip_id\"] = trip_to_route_lookup[\"trip_id\"].astype(str)\n",
        "\n",
        "#I believe this may have been resolved by splitting route_short_name into x and y\n",
        "# # Drop route_short_name from df_valid if it exists to avoid merge collision\n",
        "# if \"route_short_name\" in df_valid.columns:\n",
        "#     df_valid = df_valid.drop(columns=[\"route_short_name\"])\n",
        "\n",
        "trip_to_route_lookup[\"route_id_trips\"] = trip_to_route_lookup[\"route_id\"].copy()\n",
        "trip_to_route_lookup[\"route_short_name_trips\"] = trip_to_route_lookup[\"route_short_name\"].copy()\n",
        "\n",
        "\n",
        "# Merge to patch in route_short_name\n",
        "df_valid = df_valid.merge(trip_to_route_lookup[[\"trip_id\", \"route_id_trips\", \"route_short_name_trips\"]], on=\"trip_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "OqqwH0fEsueY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.columns"
      ],
      "metadata": {
        "id": "b90S_rscPUns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.head(10)"
      ],
      "metadata": {
        "id": "seP73BFMh4oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Build a lookup table\n",
        "route_lookup = df_valid[[\"vehicle_id\", \"timestamp\", \"route_short_name\"]].copy()\n",
        "route_lookup[\"timestamp\"] = pd.to_datetime(route_lookup[\"timestamp\"], utc=True)\n"
      ],
      "metadata": {
        "id": "cRWfXaRS91JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full.info()"
      ],
      "metadata": {
        "id": "HRJwaL_ZiKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I believe this is already resolved\n",
        "# #Step 2: Also convert timestamp in anomalies to datetime\n",
        "# df_anomalies_full[\"timestamp\"] = pd.to_datetime(df_anomalies_full[\"timestamp\"], utc=True)\n"
      ],
      "metadata": {
        "id": "DYLHwfD3_w9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a backup so we don't have to keep rerunning everything\n",
        "df_anomalies_full_copy = df_anomalies_full.copy()\n",
        "\n",
        "# #Rebuild from scratch:\n",
        "\n",
        "# df_anomalies_full_backup = pd.concat([\n",
        "    # df_anomalies_jumpgap,\n",
        "    # df_anomalies_stuck,\n",
        "    # df_anomalies_speed,\n",
        "    # df_anomalies_backtrack,\n",
        "    # df_anomalies_repeated,\n",
        "    # df_anomalies_disappear,\n",
        "    # df_anomalies_early,\n",
        "    # df_anomalies_offroute\n",
        "# ], ignore_index=True)"
      ],
      "metadata": {
        "id": "3d6YDE2t0Pmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_anomalies_full = df_anomalies_full_copy"
      ],
      "metadata": {
        "id": "QhsTvbo5ncBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… df_anomalies_full_copy has the correct anomaly types:\n",
        "repeated_points     46200  \n",
        "stuck_vehicle       30988  \n",
        "jump_or_gap          9200  â† LOST  \n",
        "off_route            6767  \n",
        "backtracking         1616  \n",
        "impossible_speed      434  â† LOST  \n",
        "disappearance          42  â† LOST  \n",
        "early_appearance        1  â† LOST  \n",
        "But after the route name merge, your current df_anomalies_full is missing the last four types.\n"
      ],
      "metadata": {
        "id": "Nb9CvVoh2mqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Merge check\n",
        "(Note: this was resolved in .v8)\n",
        "3. merge_asof Join Between df_anomalies_full and route_lookup\n",
        "- merge_result = pd.merge_asof(\n",
        "    df_anomalies_full.sort_values(\"timestamp\"),\n",
        "    route_lookup.sort_values(\"timestamp\"),\n",
        "    on=\"timestamp\",\n",
        "    by=\"vehicle_id\",\n",
        "    direction=\"nearest\",\n",
        "    ...\n",
        ")\n",
        "\n",
        "ðŸ” Potential source of corruption:\n",
        "- If route_lookup (used to get route IDs for vehicles) was built without tight trip_id / shape_id constraints, it can erroneously associate the wrong route to a vehicle.\n",
        "- merge_asof can assign the nearest route in time â€” but this may be a different shape or trip.\n",
        "\n",
        "ðŸ›‘ This is a prime suspect for inflated off_route counts."
      ],
      "metadata": {
        "id": "oN0DdJi2_UO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part of the merge investigation; can be removed later\n",
        "# # Check how many unique routes are assigned per vehicle\n",
        "# route_counts = df_anomalies_full.groupby(\"vehicle_id\")[\"route_short_name\"].nunique()\n",
        "# conflict_vehicles = route_counts[route_counts > 1].index\n",
        "\n",
        "# print(f\"ðŸš¨ Vehicles with conflicting routes: {len(conflict_vehicles)}\")\n"
      ],
      "metadata": {
        "id": "xrGszm1OuPkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Diagnostics Step 1: See how many non-null entries are already in merge_subset[\"route_short_name\"]\n",
        "# print(\"ðŸ”Ž Pre-merge route_short_name stats in merge_subset:\")\n",
        "# print(merge_subset[\"route_short_name\"].value_counts(dropna=False).head(10))\n",
        "# print(\"Total non-null route_short_name:\", merge_subset[\"route_short_name\"].notna().sum())\n"
      ],
      "metadata": {
        "id": "aGBGYwTO3IRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Diagnostics Step 2: Perform the exact merge but using a temporary suffix to compare side-by-side\n",
        "# merged_temp = pd.merge(\n",
        "#     merge_subset,\n",
        "#     route_lookup_clean[[\"vehicle_id\", \"timestamp\", \"route_short_name\"]],\n",
        "#     on=[\"vehicle_id\", \"timestamp\"],\n",
        "#     how=\"left\",\n",
        "#     suffixes=(\"_original\", \"_lookup\")\n",
        "# )\n"
      ],
      "metadata": {
        "id": "rw-tQHbx3V9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Diagnostics Step 3: Check how often the two columns agree, disagree, or one is missing\n",
        "# both_present = merged_temp[\"route_short_name_original\"].notna() & merged_temp[\"route_short_name_lookup\"].notna()\n",
        "# same = merged_temp[\"route_short_name_original\"] == merged_temp[\"route_short_name_lookup\"]\n",
        "# disagree = both_present & ~same\n",
        "\n",
        "# print(\"âœ… Both present:\", both_present.sum())\n",
        "# print(\"âœ… Agreeing values:\", (same & both_present).sum())\n",
        "# print(\"âŒ Disagreeing values:\", disagree.sum())\n"
      ],
      "metadata": {
        "id": "ofIdRXG33clL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Diagnostics Step 4: View some of the disagreements\n",
        "# print(\"âš ï¸ Sample rows where route_short_name disagrees:\")\n",
        "# display(merged_temp.loc[disagree, [\"vehicle_id\", \"timestamp\", \"route_short_name_original\", \"route_short_name_lookup\"]].head(10))\n"
      ],
      "metadata": {
        "id": "DqnRj1AG3kPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Diagnostics Step 5: Inspect which source is more complete\n",
        "# print(\"Original non-null:\", merged_temp[\"route_short_name_original\"].notna().sum())\n",
        "# print(\"Lookup non-null:\", merged_temp[\"route_short_name_lookup\"].notna().sum())\n"
      ],
      "metadata": {
        "id": "LhVg3sl63ocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full.head()"
      ],
      "metadata": {
        "id": "L0C4nXqwPx8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's redo the merging *.v8.2\n",
        "#Actually... not sure why we were doing this at all\n",
        "#This is from *.v8 addition, basically imposing exact matches on the merge, opposed to an earlier version that would use merge_asof and perform nearest matching\n",
        "# ðŸ§¼ Step 1: Clean merge inputs\n",
        "valid_rows = df_anomalies_full[\"timestamp\"].notna()\n",
        "merge_subset = df_anomalies_full.loc[valid_rows].copy()\n",
        "route_lookup_clean = route_lookup[route_lookup[\"timestamp\"].notna()].copy()\n",
        "\n",
        "# ðŸ”§ Ensure matching dtypes\n",
        "merge_subset[\"vehicle_id\"] = merge_subset[\"vehicle_id\"].astype(str)\n",
        "route_lookup_clean[\"vehicle_id\"] = route_lookup_clean[\"vehicle_id\"].astype(str)"
      ],
      "metadata": {
        "id": "SwT4N9vaYMK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_subset.columns"
      ],
      "metadata": {
        "id": "r1n9KiUrdIIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_lookup_clean.columns"
      ],
      "metadata": {
        "id": "esnxgMaidfaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge_redux = merge_subset.merge(route_lookup_clean[['vehicle_id', 'timestamp']])"
      ],
      "metadata": {
        "id": "UjzorqMQZsA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #This is the *.v8 addition, basically imposing exact matches on the merge, opposed to an earlier version that would use merge_asof and perform nearest matching\n",
        "# # ðŸ§¼ Step 1: Clean merge inputs\n",
        "# valid_rows = df_anomalies_full[\"timestamp\"].notna()\n",
        "# merge_subset = df_anomalies_full.loc[valid_rows].copy()\n",
        "# route_lookup_clean = route_lookup[route_lookup[\"timestamp\"].notna()].copy()\n",
        "\n",
        "# # ðŸ”§ Ensure matching dtypes\n",
        "# merge_subset[\"vehicle_id\"] = merge_subset[\"vehicle_id\"].astype(str)\n",
        "# route_lookup_clean[\"vehicle_id\"] = route_lookup_clean[\"vehicle_id\"].astype(str)\n",
        "\n",
        "# # âœ… Drop empty original route column\n",
        "# merge_subset = merge_subset.drop(columns=[\"route_short_name\"], errors=\"ignore\")\n",
        "\n",
        "# # ðŸ” Step 2: Exact merge\n",
        "# merged = pd.merge(\n",
        "#     merge_subset,\n",
        "#     route_lookup_clean[[\"vehicle_id\", \"timestamp\", \"route_short_name\"]],\n",
        "#     on=[\"vehicle_id\", \"timestamp\"],\n",
        "#     how=\"left\"\n",
        "# )\n",
        "\n",
        "# # ðŸ” Step 3: Patch back into df_anomalies_full\n",
        "# route_name_map = merged.set_index(merge_subset.index)[\"route_short_name\"]\n",
        "# df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].combine_first(route_name_map)\n",
        "\n",
        "# # âœ… Step 4: Final check\n",
        "# print(\"âœ… Remaining missing route names:\", df_anomalies_full[\"route_short_name\"].isna().sum())\n"
      ],
      "metadata": {
        "id": "IwlafUWj5j2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####*.v8.2 250731-1630 note: not sure why we were cleaning the data above\n",
        "and I think this merging may have caused some issues, so skipping it for now"
      ],
      "metadata": {
        "id": "W8vDlM3Cmk2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full[df_anomalies_full[\"route_short_name\"].isna()].sample(5)\n"
      ],
      "metadata": {
        "id": "lEZGO2YPt1ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full.isna().sum()"
      ],
      "metadata": {
        "id": "ZTaJu9Ajm4fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Debugging waypoint 2507251634"
      ],
      "metadata": {
        "id": "jEi00-AOuKhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full['route_short_name'].value_counts()"
      ],
      "metadata": {
        "id": "yScpQydTnCEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Presently there are no unknowns or nulls, so we don't have to worry about this\n",
        "# # Treat 'Unknown' as null\n",
        "# df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].replace(\"Unknown\", pd.NA)\n",
        "\n",
        "# # Only apply fallback if the lookup column is still present\n",
        "# if \"route_short_name_from_lookup\" in df_anomalies_full.columns:\n",
        "#     df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].combine_first(\n",
        "#         df_anomalies_full[\"route_short_name_from_lookup\"]\n",
        "#     )\n",
        "\n",
        "# # Confirm\n",
        "# print(\"Remaining missing route names:\", df_anomalies_full[\"route_short_name\"].isna().sum())\n",
        "# print(df_anomalies_full[\"route_short_name\"].value_counts(dropna=False).head())\n"
      ],
      "metadata": {
        "id": "SnJkVVXxNRlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same as above cell\n",
        "# # Confirm null replacement and summary\n",
        "# df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].replace(\"Unknown\", pd.NA)\n",
        "\n",
        "# # Summary diagnostics\n",
        "# print(\"Remaining missing route names:\", df_anomalies_full[\"route_short_name\"].isna().sum())\n",
        "# print(df_anomalies_full[\"route_short_name\"].value_counts(dropna=False).head())\n"
      ],
      "metadata": {
        "id": "hTEe4Nt8-gGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checkpoint anomaly type breakdown:\")\n",
        "print(df_anomalies_full[\"anomaly_type\"].value_counts(dropna=False))\n",
        "\n",
        "print(\"Checkpoint anomaly type breakdown (copy):\")\n",
        "print(df_anomalies_full_copy[\"anomaly_type\"].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "kDyn6nbHlJH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique types in anomaly_type:\")\n",
        "print(set(type(val) for val in df_anomalies_full[\"anomaly_type\"].unique()))\n"
      ],
      "metadata": {
        "id": "fgN1-Q5IqF3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full anomaly types (raw):\", df_anomalies_full[\"anomaly_type\"].unique())\n",
        "print(\"Full anomaly types (with types):\", [(val, type(val)) for val in df_anomalies_full[\"anomaly_type\"].unique()])\n"
      ],
      "metadata": {
        "id": "HJi8k9ZkqlK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_detectors)"
      ],
      "metadata": {
        "id": "zb3JezlzjD2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full[\"anomaly_type\"].unique()"
      ],
      "metadata": {
        "id": "x6XYLSAJjdkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean anomaly type column\n",
        "df_temp = df_anomalies_full.copy()\n",
        "df_temp[\"route_short_name\"] = df_temp[\"route_short_name\"].fillna(\"(Missing)\")\n",
        "df_temp[\"anomaly_type_clean\"] = df_temp[\"anomaly_type\"].astype(str)\n",
        "\n",
        "# Step 2: Build pivot table\n",
        "pivot = df_temp.pivot_table(\n",
        "    index=\"route_short_name\",\n",
        "    columns=\"anomaly_type_clean\",\n",
        "    aggfunc=\"size\",\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Step 3: Manually reindex using the full list\n",
        "expected_columns = [\n",
        "    \"repeated_points\", \"stuck_vehicle\", \"jump_or_gap\", \"off_route\",\n",
        "    \"backtracking\", \"impossible_speed\", \"disappearance\", \"early_appearance\"\n",
        "]\n",
        "pivot = pivot.reindex(columns=expected_columns, fill_value=0)\n",
        "\n",
        "# Step 4: Diagnostics\n",
        "print(\"âœ… Total in pivot:\", pivot.sum().sum())\n",
        "print(\"âœ… Total in df:\", df_anomalies_full[\"anomaly_type\"].notna().sum())\n",
        "print(\"âœ… Columns in pivot:\", pivot.columns.tolist())\n"
      ],
      "metadata": {
        "id": "1roQaCTbqvui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Fix Summary (for documentation or future review):\n",
        "- Problem cause: pivot_table(...).reindex(columns=sorted(...)) introduced TypeError due to sorting mixed types (possibly hidden NaNs or object-type weirdness in internal index representation).\n",
        "\n",
        "- Confirmed all anomaly_type values were str, but reindexing still failed â€” indicating a more subtle index-level mismatch.\n",
        "\n",
        "Solution: Explicitly cast anomaly_type to string and reindex with a predefined list of expected columns."
      ],
      "metadata": {
        "id": "ssdO4P-rrEeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot"
      ],
      "metadata": {
        "id": "fDQExTBPq6wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second pivot table to try looking at things other than route_short_name\n",
        "# Step 1: Clean anomaly type column\n",
        "df_temp2 = df_anomalies_full.copy()\n",
        "df_temp2[\"trip_id\"] = df_temp2[\"trip_id\"].fillna(\"(Missing)\")\n",
        "df_temp2[\"anomaly_type_clean\"] = df_temp2[\"anomaly_type\"].astype(str)\n",
        "\n",
        "# Step 2: Build pivot table\n",
        "pivot2 = df_temp2.pivot_table(\n",
        "    index=\"trip_id\",\n",
        "    columns=\"anomaly_type_clean\",\n",
        "    aggfunc=\"size\",\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Step 3: Manually reindex using the full list\n",
        "expected_columns = [\n",
        "    \"repeated_points\", \"stuck_vehicle\", \"jump_or_gap\", \"off_route\",\n",
        "    \"backtracking\", \"impossible_speed\", \"disappearance\", \"early_appearance\"\n",
        "]\n",
        "pivot2 = pivot2.reindex(columns=expected_columns, fill_value=0)\n",
        "\n",
        "# Step 4: Diagnostics\n",
        "print(\"âœ… Total in pivot:\", pivot2.sum().sum())\n",
        "print(\"âœ… Total in df:\", df_anomalies_full[\"anomaly_type\"].notna().sum())\n",
        "print(\"âœ… Columns in pivot:\", pivot2.columns.tolist())"
      ],
      "metadata": {
        "id": "P2qIKHqSoUCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot2"
      ],
      "metadata": {
        "id": "bssuXVFyozDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Debuggin in *.v8, to do:\n",
        "Thanks for the clear context and diagnostic output â€” this is an important signal, and you're absolutely right to question it.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ” What's Going Wrong\n",
        "\n",
        "The anomaly types:\n",
        "\n",
        "* `\"jump_or_gap\"`, `\"impossible_speed\"`, `\"disappearance\"`, `\"early_appearance\"`\n",
        "\n",
        "are **only appearing under `(Missing)` routes**, while all known route numbers (like `1`, `16`, `66`) show `0` in those columns.\n",
        "\n",
        "Yet we know these anomaly types **should be assigned to real routes** in many cases.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Root Cause\n",
        "\n",
        "Those anomalies **likely didn't get their `route_short_name` correctly assigned** during the earlier merge. Here's why:\n",
        "\n",
        "* Many of these anomaly types are generated from **timestamp-level snapshot comparisons**, often between vehiclesâ€™ **first or last records**, or **with subtle time gaps**.\n",
        "* If the timestamp didnâ€™t match exactly with a route in the `route_lookup`, and you removed `merge_asof`, they **werenâ€™t matched at all**.\n",
        "* So they fell through with missing `route_short_name`, and got lumped into the `\"(Missing)\"` category in the pivot.\n",
        "\n",
        "In short:\n",
        "\n",
        "> You now get only **exact route matches**, which means **no match = Missing = misattributed anomalies**.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Recommended Fix\n",
        "\n",
        "To preserve strict merge logic *but still capture route info for jump/disappearance-type anomalies*, you need a **separate merge strategy** for those rows. Here's how:\n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸ”„ Updated Strategy\n",
        "\n",
        "1. **Retain your current exact-match logic** for most anomalies (good for accuracy).\n",
        "2. **For rows that still have no route**, use a **second fallback join**:\n",
        "\n",
        "   * Join on `(vehicle_id, rounded timestamp)` with small Â± tolerance via *manual logic*, **not `merge_asof`**.\n",
        "   * Or pre-fill route for vehicles known to follow the same route across time.\n",
        "\n",
        "Alternatively, if you're okay adding a **hybrid merge strategy**, then:\n",
        "\n",
        "* First: `merge` (strict)\n",
        "* Then: `merge_asof` only on rows that are still `NaN` for route\n",
        "* Flag those rows as `fallback_match = True` for auditability\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš§ Immediate Diagnostic You Can Try\n",
        "\n",
        "Letâ€™s confirm our theory first.\n",
        "\n",
        "Run this:\n",
        "\n",
        "```python\n",
        "missing_routes_df = df_anomalies_full[df_anomalies_full[\"route_short_name\"].isna()]\n",
        "print(\"ðŸš« Missing route entries by anomaly type:\")\n",
        "print(missing_routes_df[\"anomaly_type\"].value_counts())\n",
        "```\n",
        "\n",
        "Youâ€™ll probably see that all the `jump_or_gap`, `impossible_speed`, etc. live in this `missing_routes_df`.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ› ï¸ Optional Patch to Recover These\n",
        "\n",
        "If youâ€™re willing to use a **limited scope fallback merge**, try this:\n",
        "\n",
        "```python\n",
        "# Find rows that still have missing route\n",
        "missing_route_rows = df_anomalies_full[\"route_short_name\"].isna()\n",
        "\n",
        "# Attempt fallback merge on these using merge_asof (only here)\n",
        "fallback_merge = pd.merge_asof(\n",
        "    df_anomalies_full.loc[missing_route_rows].sort_values(\"timestamp\"),\n",
        "    route_lookup_clean.sort_values(\"timestamp\"),\n",
        "    on=\"timestamp\",\n",
        "    by=\"vehicle_id\",\n",
        "    direction=\"nearest\",\n",
        "    tolerance=pd.Timedelta(\"60s\")\n",
        ")\n",
        "\n",
        "# Patch only where it's still missing\n",
        "df_anomalies_full.loc[missing_route_rows, \"route_short_name\"] = fallback_merge[\"route_short_name\"]\n",
        "```\n",
        "\n",
        "âš ï¸ This preserves your integrity for the main dataset, and **only relaxes the constraint for those otherwise unclassified rows**.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like help implementing that fallback merge and visual diagnostic?\n"
      ],
      "metadata": {
        "id": "Za5wbMONFRJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Note: this was debugging notes from â‰¤*.v6 **\n",
        "\n",
        "Interestingly, the (Missing) route_short_name has the only instances of jump_or_gap, impossible_speed, disappearance, early_appearance\n",
        "\n",
        "\n",
        "Thatâ€™s a critical diagnostic insight â€” and it likely explains why those anomaly types kept disappearing in earlier pivot tables:\n",
        "\n",
        "ðŸ” Root Cause:\n",
        "By default, pivot_table uses only the combination of index and columns values that exist. If all rows for some anomaly types are associated with a missing route_short_name (NaN), and you drop or exclude these during preprocessing, those types vanish from the pivot.\n",
        "\n",
        "When you did:\n",
        "\n",
        "df[\"route_short_name\"] = df[\"route_short_name\"].fillna(\"(Missing)\")\n",
        "\n",
        "You ensured that:\n",
        "\n",
        "Those rows were not excluded from the index.\n",
        "\n",
        "Their anomaly types were included and preserved in the pivot.\n",
        "\n",
        "âœ… Implications:\n",
        "You must preserve (Missing) rows if you want a complete count across all anomaly types.\n",
        "\n",
        "If you filter by route, or use .dropna() on route_short_name, anomaly types with only missing routes will be silently excluded.\n",
        "\n",
        "When you view a pivot per route, types like jump_or_gap, impossible_speed, and disappearance may seem to â€œdisappearâ€ â€” but theyâ€™re actually only found in the (Missing) bucket.\n",
        "\n",
        "---\n",
        "To ensure full awareness in reporting or visualizing these anomaly types, do one of the following:\n",
        "\n",
        "Always include a (Missing) category in route-level pivots.\n",
        "\n",
        "Add a diagnostic count like:\n"
      ],
      "metadata": {
        "id": "5Bar0BxUrkdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anomalies with missing route names by type:\")\n",
        "print(df_anomalies_full[df_anomalies_full[\"route_short_name\"].isna()][\"anomaly_type\"].value_counts())\n"
      ],
      "metadata": {
        "id": "_DVWQ1W1r0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomalies_full[\"anomaly_type\"].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "6OOThv6NN25m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot in Folium"
      ],
      "metadata": {
        "id": "AaLiLOnGSB5J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ORsBn80ZXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I don't think this is actually useful at this point\n",
        "# # Step 1: Create a working copy\n",
        "# df_plot_base = df_anomalies_full.copy()\n",
        "\n",
        "# # Step 2: Drop rows with missing coordinates or timestamps\n",
        "# df_plot_base = df_plot_base.dropna(subset=[\"latitude\", \"longitude\", \"timestamp_collected\"])\n",
        "\n",
        "# # Step 3: Ensure coordinate and timestamp types are correct\n",
        "# df_plot_base[\"latitude\"] = pd.to_numeric(df_plot_base[\"latitude\"], errors=\"coerce\")\n",
        "# df_plot_base[\"longitude\"] = pd.to_numeric(df_plot_base[\"longitude\"], errors=\"coerce\")\n",
        "# df_plot_base[\"timestamp_collected\"] = pd.to_datetime(df_plot_base[\"timestamp_collected\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "# # Step 4: Optional â€” Filter for anomalies with known route_short_name\n",
        "# df_plot_base = df_plot_base[df_plot_base[\"route_short_name\"].notna()]\n",
        "\n",
        "# # Step 5: Optional â€” Save a filtered subset by anomaly type, route, or vehicle for visualization\n",
        "# # You can update these filters dynamically for an interactive tool\n",
        "# selected_anomaly_type = \"stuck_vehicle\"\n",
        "# selected_route = \"66\"\n",
        "# # Example: match on anomaly type and route\n",
        "# df_plot_filtered = df_plot_base[\n",
        "#     (df_plot_base[\"anomaly_type\"] == selected_anomaly_type) &\n",
        "#     (df_plot_base[\"route_short_name\"] == selected_route)\n",
        "# ]\n",
        "\n",
        "# # Preview result\n",
        "# print(f\"ðŸ—ºï¸ Plotting subset for anomaly: '{selected_anomaly_type}' on route '{selected_route}'\")\n",
        "# print(df_plot_filtered[[\"vehicle_id\", \"timestamp_collected\", \"latitude\", \"longitude\", \"anomaly_type\"]].head())\n"
      ],
      "metadata": {
        "id": "uWcU42cktY-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use full anomaly dataframe\n",
        "df_full2 = df_anomalies_full.copy()\n",
        "\n",
        "# Ensure lat/lon are floats\n",
        "df_full2[\"latitude\"] = pd.to_numeric(df_full2[\"latitude\"], errors=\"coerce\")\n",
        "df_full2[\"longitude\"] = pd.to_numeric(df_full2[\"longitude\"], errors=\"coerce\")\n",
        "\n",
        "# Ensure timestamp_collected is datetime\n",
        "df_full2[\"timestamp_collected\"] = pd.to_datetime(df_full2[\"timestamp_collected\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "# Compute diffs for position and time\n",
        "df_full2[\"lat_diff\"] = df_full2.groupby(\"vehicle_id\")[\"latitude\"].diff()\n",
        "df_full2[\"lon_diff\"] = df_full2.groupby(\"vehicle_id\")[\"longitude\"].diff()\n",
        "df_full2[\"jump_dist\"] = (df_full2[\"lat_diff\"]**2 + df_full2[\"lon_diff\"]**2)**0.5\n",
        "df_full2[\"time_diff\"] = df_full2.groupby(\"vehicle_id\")[\"timestamp_collected\"].diff().dt.total_seconds()\n",
        "\n",
        "# Label jump and disappearance events using anomaly_params\n",
        "df_full2[\"is_jump\"] = df_full2[\"jump_dist\"] > anomaly_params[\"JUMP_DISTANCE_THRESHOLD\"]\n",
        "df_full2[\"is_disappearance\"] = df_full2[\"time_diff\"] > anomaly_params[\"DISAPPEARANCE_TIME_THRESHOLD\"]\n",
        "\n",
        "# Create enriched jump DataFrame\n",
        "jumps_df = df_full2[df_full2[\"is_jump\"]].copy()\n",
        "jumps_df[\"lat_prev\"] = df_full2.groupby(\"vehicle_id\")[\"latitude\"].shift()\n",
        "jumps_df[\"lon_prev\"] = df_full2.groupby(\"vehicle_id\")[\"longitude\"].shift()\n",
        "jumps_df[\"timestamp_prev\"] = df_full2.groupby(\"vehicle_id\")[\"timestamp_collected\"].shift()\n",
        "jumps_df[\"timestamp_curr\"] = jumps_df[\"timestamp_collected\"]\n",
        "\n",
        "# Filter to vehicles with enough jumps\n",
        "jump_counts = jumps_df[\"vehicle_id\"].value_counts()\n",
        "keep_jumpers = jump_counts[jump_counts >= anomaly_params[\"MIN_JUMP_COUNT_PER_VEHICLE\"]].index\n",
        "jumps_df = jumps_df[jumps_df[\"vehicle_id\"].isin(keep_jumpers)]\n",
        "\n",
        "# Base frame of jumpers for disappearance/reappearance\n",
        "df_jumpers_only = df_full2[df_full2[\"vehicle_id\"].isin(keep_jumpers)].copy()\n",
        "\n",
        "# Identify disappearance and reappearance points\n",
        "disappear_df = df_jumpers_only[df_jumpers_only[\"is_disappearance\"]].copy()\n",
        "is_reappear = df_jumpers_only[\"is_disappearance\"].shift(-1).fillna(False)\n",
        "reappear_df = df_jumpers_only[is_reappear].copy()\n",
        "\n",
        "# Summary of Key Changes:\n",
        "# âœ… Replaces JUMP_DISTANCE_THRESHOLD, DISAPPEARANCE_TIME_THRESHOLD, and MIN_JUMP_COUNT_PER_VEHICLE with their anomaly_params equivalents.\n",
        "# âœ… Maintains existing structure and logic.\n",
        "# âœ… Compatible with all downstream Folium plotting layers.\n"
      ],
      "metadata": {
        "id": "FpulIq8jvJfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I believe this can be depricated\n",
        "# # that traceback confirms the immediate issue: after merging into shapes_df, the column route_short_name is not present, likely due to:\n",
        "\n",
        "# # A failed merge (i.e. no matching shape_id values across shapes_df and shape_route_map).\n",
        "\n",
        "# # The column being named something else (e.g. a merge conflict that renamed it, or a typo).\n",
        "\n",
        "# # An earlier shapes_df overwrite or clean-up step that removed key columns.\n",
        "\n",
        "# print(\"ðŸ” Columns in shapes_df:\", shapes_df.columns.tolist())\n",
        "\n",
        "# # Check if shape_route_map exists and contains route_short_name\n",
        "# if 'shape_route_map' in globals():\n",
        "#     print(\"âœ… Columns in shape_route_map:\", shape_route_map.columns.tolist())\n",
        "#     print(\"âœ… Sample shape_route_map rows:\")\n",
        "#     display(shape_route_map.head())\n",
        "# else:\n",
        "#     print(\"âŒ shape_route_map does not exist\")\n",
        "\n",
        "# # Check shape_id overlap\n",
        "# if 'shapes_df' in globals():\n",
        "#     print(\"ðŸ§ª shape_id in shapes_df:\", shapes_df[\"shape_id\"].nunique())\n",
        "#     print(\"ðŸ§ª shape_id in shape_route_map:\", shape_route_map[\"shape_id\"].nunique())\n",
        "#     print(\"ðŸ§ª Overlapping shape_ids:\", len(set(shapes_df[\"shape_id\"]).intersection(shape_route_map[\"shape_id\"])))\n"
      ],
      "metadata": {
        "id": "aZEFDtSsyCaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Extract shape_id to route_id from trips_df\n",
        "shape_to_route = trips[[\"shape_id\", \"route_id\"]].drop_duplicates()\n",
        "# trips.txt defines which route uses which shape.\n",
        "# shape_to_route gives a clean 1:1 or 1:many mapping.\n",
        "\n",
        "#Step 2: Map route_id to route_short_name from routes_df\n",
        "route_id_to_name = routes[[\"route_id\", \"route_short_name\"]]\n",
        "#Brings in human-readable route names like \"1\", \"10\", \"66\".\n",
        "\n",
        "#Step 3: Merge to associate shape_id with route_short_name\n",
        "shape_route_map = shape_to_route.merge(route_id_to_name, on=\"route_id\", how=\"left\")\n",
        "#You now have a table with: shape_id, route_id, and route_short_name\n",
        "\n",
        "shapes_copy = shapes.copy()\n",
        "\n",
        "# Step 4: Drop preexisting route_short_name to avoid merge conflicts\n",
        "if \"route_short_name\" in shapes_copy.columns:\n",
        "    shapes_copy.drop(columns=[\"route_short_name\"], inplace=True)\n",
        "\n",
        "# Step 4 (retry): Merge into shapes_df\n",
        "shapes_df = shapes_copy.merge(shape_route_map, on=\"shape_id\", how=\"left\")\n",
        "\n",
        "# Step 5: Confirm structure\n",
        "assert shapes_df[\"route_short_name\"].notna().all(), \"âŒ Some shapes are missing route names!\"\n",
        "print(\"âœ… Final columns in shapes_df:\", shapes_df.columns.tolist())\n",
        "display(shapes_df[[\"shape_id\", \"route_short_name\"]].drop_duplicates().head())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uRqmK5pw2II7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I believe this can be depricated\n",
        "# # Next Step: Clean up unnecessary suffix columns\n",
        "# # Since the final route_short_name column is already correct, and we donâ€™t need the duplicated *_x and *_y columns, you can safely drop them:\n",
        "# # Clean up leftover duplicate columns\n",
        "# cols_to_drop = [col for col in shapes_df.columns if col.endswith(\"_x\") or col.endswith(\"_y\")]\n",
        "# shapes_df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# # Final confirmation\n",
        "# print(\"âœ… Cleaned columns in shapes_df:\", shapes_df.columns.tolist())\n",
        "# display(shapes_df[[\"shape_id\", \"route_short_name\"]].drop_duplicates().head())\n"
      ],
      "metadata": {
        "id": "_xtaQawtzQ38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Current shapes_df Columns:\n",
        "- shape_id: Unique shape identifier\n",
        "- shape_pt_lat, shape_pt_lon: GPS points to trace the route shape\n",
        "- shape_pt_sequence: Order of points to draw the shape\n",
        "- shape_dist_traveled: Optional, often used to interpolate positions\n",
        "- route_id, route_short_name: Now both included â€” route_short_name is the human-readable one youâ€™ll want for plotting and filtering"
      ],
      "metadata": {
        "id": "n_5xyMHm3WBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create base map ---\n",
        "mymap = Map(location=[35.0844, -106.6504], zoom_start=12)\n",
        "\n",
        "# --- Define color map for routes ---\n",
        "route_names = sorted(df_anomalies_full['route_short_name'].dropna().unique())\n",
        "cmap = plt.get_cmap(\"tab20\", len(route_names))\n",
        "color_map = {route: mcolors.to_hex(cmap(i)) for i, route in enumerate(route_names)}\n",
        "\n",
        "# --- Plot anomaly markers by route and type ---\n",
        "for (route, anomaly), subset in df_anomalies_full.groupby([\"route_short_name\", \"anomaly_type\"]):\n",
        "    group = FeatureGroup(name=f\"{route} â€“ {anomaly}\", show=False)\n",
        "    for _, row in subset.iterrows():\n",
        "        if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]):\n",
        "            popup = (\n",
        "                f\"Anomaly: {anomaly}<br>\"\n",
        "                f\"Route: {route}<br>\"\n",
        "                f\"Vehicle: {row.get('vehicle_id', 'N/A')}<br>\"\n",
        "                f\"Timestamp: {row.get('timestamp_collected', 'N/A')}\"\n",
        "            )\n",
        "            CircleMarker(\n",
        "                location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "                radius=4,\n",
        "                color=color_map.get(route, \"black\"),\n",
        "                fill=True,\n",
        "                fill_opacity=0.9,\n",
        "                popup=popup\n",
        "            ).add_to(group)\n",
        "    group.add_to(mymap)\n",
        "\n",
        "# --- Plot jump lines and markers ---\n",
        "if 'jumps_df' in globals() and not jumps_df.empty:\n",
        "    for route, group_df in jumps_df.groupby(\"route_short_name\"):\n",
        "        jump_line_group = FeatureGroup(name=f\"{route} â€“ Jump Lines\", show=False)\n",
        "        jump_point_group = FeatureGroup(name=f\"{route} â€“ Jump Start/End\", show=False)\n",
        "        for _, row in group_df.iterrows():\n",
        "            if all(pd.notna([row[\"lat_prev\"], row[\"lon_prev\"], row[\"latitude\"], row[\"longitude\"]])):\n",
        "                start = [row[\"lat_prev\"], row[\"lon_prev\"]]\n",
        "                end = [row[\"latitude\"], row[\"longitude\"]]\n",
        "                vehicle = row.get(\"vehicle_id\", \"N/A\")\n",
        "                t_prev = row.get(\"timestamp_prev\", \"N/A\")\n",
        "                t_curr = row.get(\"timestamp_curr\", \"N/A\")\n",
        "\n",
        "                PolyLine([start, end], color=\"orange\", weight=2,\n",
        "                         tooltip=f\"Vehicle {vehicle} jump\").add_to(jump_line_group)\n",
        "\n",
        "                CircleMarker(location=start, radius=4, color=\"blue\", fill=True,\n",
        "                             fill_opacity=0.9, popup=f\"START â€“ {vehicle}<br>{t_prev}\").add_to(jump_point_group)\n",
        "                CircleMarker(location=end, radius=4, color=\"purple\", fill=True,\n",
        "                             fill_opacity=0.9, popup=f\"END â€“ {vehicle}<br>{t_curr}\").add_to(jump_point_group)\n",
        "        jump_line_group.add_to(mymap)\n",
        "        jump_point_group.add_to(mymap)\n",
        "\n",
        "# --- Plot disappearances ---\n",
        "if 'disappear_df' in globals() and not disappear_df.empty:\n",
        "    for route, group_df in disappear_df.groupby(\"route_short_name\"):\n",
        "        disappear_group = FeatureGroup(name=f\"{route} â€“ Disappearances\", show=False)\n",
        "        for _, row in group_df.iterrows():\n",
        "            if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]):\n",
        "                Marker(\n",
        "                    location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "                    icon=Icon(color=\"red\", icon=\"times-circle\", prefix=\"fa\"),\n",
        "                    tooltip=f\"Vehicle {row.get('vehicle_id')} disappeared<br>{row.get('timestamp_collected')}\"\n",
        "                ).add_to(disappear_group)\n",
        "        disappear_group.add_to(mymap)\n",
        "\n",
        "# --- Plot reappearances ---\n",
        "if 'reappear_df' in globals() and not reappear_df.empty:\n",
        "    for route, group_df in reappear_df.groupby(\"route_short_name\"):\n",
        "        reappear_group = FeatureGroup(name=f\"{route} â€“ Reappearances\", show=False)\n",
        "        for _, row in group_df.iterrows():\n",
        "            if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]):\n",
        "                Marker(\n",
        "                    location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "                    icon=Icon(color=\"green\", icon=\"check-circle\", prefix=\"fa\"),\n",
        "                    tooltip=f\"Vehicle {row.get('vehicle_id')} reappeared<br>{row.get('timestamp_collected')}\"\n",
        "                ).add_to(reappear_group)\n",
        "        reappear_group.add_to(mymap)\n",
        "\n",
        "# --- Plot route shapes ---\n",
        "if 'shapes_df' in globals() and not shapes_df.empty:\n",
        "    for route, group in shapes_df.groupby(\"route_short_name\"):\n",
        "        route_group = FeatureGroup(name=f\"{route} â€“ Route Shape\", show=False)\n",
        "        for shape_id, shape_data in group.groupby(\"shape_id\"):\n",
        "            shape_data = shape_data.sort_values(\"shape_pt_sequence\")\n",
        "            latlons = list(zip(shape_data[\"shape_pt_lat\"], shape_data[\"shape_pt_lon\"]))\n",
        "            PolyLine(\n",
        "                locations=latlons,\n",
        "                color=color_map.get(route, \"gray\"),\n",
        "                weight=2,\n",
        "                opacity=0.6,\n",
        "                popup=f\"Route {route} | Shape {shape_id}\"\n",
        "            ).add_to(route_group)\n",
        "        route_group.add_to(mymap)\n",
        "\n",
        "# --- Add layer controls ---\n",
        "LayerControl(collapsed=False).add_to(mymap)\n",
        "\n",
        "# --- Display the map ---\n",
        "mymap"
      ],
      "metadata": {
        "id": "d3GY0KoE9vDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YX3ChyLw9vkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}