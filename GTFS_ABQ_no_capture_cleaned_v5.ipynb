{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+KjFsNcpO5hgLrb35RhH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DDDS18-GTFS/ddds.18.capstone/blob/dev.Andrew/GTFS_ABQ_no_capture_cleaned_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Libraries"
      ],
      "metadata": {
        "id": "KI4QbV7NQx9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_t8RdRcMqIZ"
      },
      "outputs": [],
      "source": [
        "#Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "from shapely.geometry import Point\n",
        "from geopy.distance import geodesic\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "# from folium import Map, FeatureGroup, CircleMarker, PolyLine, Marker, Icon, LayerControl\n",
        "from matplotlib import colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import folium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBelZMehH740"
      },
      "outputs": [],
      "source": [
        "#These are defined in the Anomaly Detection section cells individually, but we will want to define them more easily, and may move it up here at some point\n",
        "# # ---------------------\n",
        "# # Threshold parameters\n",
        "# # ---------------------\n",
        "\n",
        "# # Distance (in degrees) that counts as a GPS \"jump\" (this is greater than the average stop distance)\n",
        "# JUMP_DISTANCE_THRESHOLD = 0.005  # â‰ˆ 500 meters\n",
        "\n",
        "# # Time gap (in seconds) that counts as a disappearance\n",
        "# DISAPPEARANCE_TIME_THRESHOLD = 300  # 5 minutes\n",
        "\n",
        "# # Only show vehicles with at least this many jumps\n",
        "# MIN_JUMP_COUNT_PER_VEHICLE = 1\n",
        "\n",
        "# # Number of vehicles to sample for map clarity\n",
        "# NUM_VEHICLES_TO_SAMPLE = 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the RT snapshot"
      ],
      "metadata": {
        "id": "FC5kUKG6Qp9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 1a: Load and Inspect the New Snapshot\n",
        "new_snapshot_path = \"/content/cabq_gtfs_snapshots_20250722_1415.csv\"\n",
        "df_new = pd.read_csv(new_snapshot_path)\n",
        "# df_new.info()\n",
        "# df_new.head(3)\n",
        "\n",
        "# #Also print the columns:\n",
        "# print(df_new.columns.tolist())\n"
      ],
      "metadata": {
        "id": "0ypQFlcHLSlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 2a: Trip ID Validity\n",
        "df_new[\"trip_id\"] = df_new[\"trip_id\"].astype(str)\n",
        "invalid_trip_ids = df_new[\"trip_id\"].isin([\"0\", \"Undetermined\", \"nan\", \"\", \"None\"]).sum()\n",
        "total_rows = len(df_new)\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"Invalid trip_ids: {invalid_trip_ids}\")\n",
        "print(f\"Percent valid: {100 * (total_rows - invalid_trip_ids) / total_rows:.2f}%\")\n"
      ],
      "metadata": {
        "id": "4fmA5kemLoOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I believe this is fully depricated and can be removed\n",
        "# #âœ… Step 3a: Route ID Normalization (Preview)\n",
        "# # We'll also check route formatting now that we know floats were a problem previously:\n",
        "\n",
        "# df_new[\"route_id\"] = df_new[\"route_id\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "# print(df_new[\"route_id\"].dropna().unique()[:10])\n"
      ],
      "metadata": {
        "id": "UW_jtP_LLw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Static data"
      ],
      "metadata": {
        "id": "rHcIyBIuQtWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 1b: Reload Static GTFS and Normalize It\n",
        "\n",
        "# Adjust if needed â€” make sure this is the static feed aligned with 2025-07-22\n",
        "gtfs_zip_path = \"/content/google_transit.zip\"\n",
        "\n",
        "with zipfile.ZipFile(gtfs_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/gtfs_static\")\n",
        "\n",
        "trips = pd.read_csv(\"/content/gtfs_static/trips.txt\", dtype=str)\n",
        "routes = pd.read_csv(\"/content/gtfs_static/routes.txt\", dtype=str)\n"
      ],
      "metadata": {
        "id": "_bLK0Q2PNDG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 2b: Filter invalid trip_ids\n",
        "invalid_trip_ids = [\"0\", \"Undetermined\", \"nan\", \"\", \"None\"]\n",
        "df_clean = df_new[~df_new[\"trip_id\"].isin(invalid_trip_ids)].copy()\n"
      ],
      "metadata": {
        "id": "hRsg9R1sNJk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 3b: Merge with trips.txt to Get route_id\n",
        "df_with_trips = df_clean.merge(trips, on=\"trip_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "QeT3MyKbNNs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 4b: Merge with routes.txt to Get Descriptive Info\n",
        "df_with_trips[\"route_id\"] = df_with_trips[\"route_id\"].astype(str)\n",
        "\n",
        "# Ensure consistent types\n",
        "trips[\"shape_id\"] = trips[\"shape_id\"].astype(str)\n",
        "routes[\"route_id\"] = routes[\"route_id\"].astype(str)\n",
        "\n",
        "df_full = df_with_trips.merge(routes, on=\"route_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "ax8zNn88NSNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 5b: Load shapes.txt\n",
        "shapes = pd.read_csv(\"/content/gtfs_static/shapes.txt\", dtype={\"shape_id\": str})\n",
        "\n",
        "# Build LineStrings for each shape_id\n",
        "shape_lines = {}\n",
        "for shape_id, group in shapes.groupby(\"shape_id\"):\n",
        "    sorted_group = group.sort_values(\"shape_pt_sequence\")\n",
        "    coords = list(zip(sorted_group[\"shape_pt_lon\"], sorted_group[\"shape_pt_lat\"]))\n",
        "    shape_lines[shape_id] = LineString(coords)\n",
        "\n",
        "# Merge trips and routes to link shape_id to route_short_name\n",
        "shape_route_map = (\n",
        "    trips.merge(routes, on=\"route_id\", how=\"left\")\n",
        "         .dropna(subset=[\"route_short_name\"])\n",
        "         .drop_duplicates(subset=[\"shape_id\"])\n",
        "         .set_index(\"shape_id\")[[\"route_id\", \"route_short_name\"]]\n",
        ")"
      ],
      "metadata": {
        "id": "FPmkslyjUqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ… Step 6b: Create a data_quality Flag\n",
        "def classify_row(row):\n",
        "    if row[\"trip_id\"] in invalid_trip_ids:\n",
        "        return \"invalid_trip_id\"\n",
        "    elif pd.isna(row[\"route_id\"]):\n",
        "        return \"missing_route_id\"\n",
        "    elif pd.isna(row[\"route_long_name\"]):\n",
        "        return \"missing_route_metadata\"\n",
        "    else:\n",
        "        return \"valid\"\n",
        "\n",
        "df_full[\"data_quality\"] = df_full.apply(classify_row, axis=1)\n",
        "print(df_full[\"data_quality\"].value_counts())\n"
      ],
      "metadata": {
        "id": "KkvElwvqNWAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the trips, routes, shapes from Static data\n",
        "with zipfile.ZipFile(gtfs_zip_path, 'r') as z:\n",
        "    trips_df = pd.read_csv(z.open(\"trips.txt\"))\n",
        "    routes_df = pd.read_csv(z.open(\"routes.txt\"))\n",
        "    # Load GTFS shapes.txt into a DataFrame\n",
        "    shapes_df = pd.read_csv(z.open(\"shapes.txt\"))"
      ],
      "metadata": {
        "id": "mobuNn50Xn93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean the RT data"
      ],
      "metadata": {
        "id": "1PdBamqvTD_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 1.1 Filter for Valid Rows\n",
        "df_valid = df_full[df_full[\"data_quality\"] == \"valid\"].copy()\n",
        "\n",
        "#ðŸ”¹ 1.2 Parse Timestamps\n",
        "df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "\n",
        "#ðŸ”¹ 1.3 Sort by Vehicle and Timestamp\n",
        "df_valid = df_valid.sort_values(by=[\"vehicle_id\", \"timestamp\"])\n",
        "\n",
        "#ðŸ”¹ 1.4 Organize by Vehicle\n",
        "#This creates a dictionary keyed by vehicle ID, each with a sorted DataFrame:\n",
        "vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "#You can confirm how many distinct vehicles youâ€™re tracking:\n",
        "print(\"Vehicle count:\", len(vehicle_groups))\n"
      ],
      "metadata": {
        "id": "L0NjFMGZTHp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Anomly Detection"
      ],
      "metadata": {
        "id": "QVXWXP9FRX1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ 1. Detect Jumps and Gaps\n",
        "\n",
        "Already implemented, but hereâ€™s the modular form:"
      ],
      "metadata": {
        "id": "EaP4VCejSudu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Detect Jumps and Gaps\n",
        "def detect_jumps_and_gaps(df, distance_threshold=250, time_threshold=90):\n",
        "    anomalies = []\n",
        "    for i in range(1, len(df)):\n",
        "        row_prev, row_curr = df.iloc[i - 1], df.iloc[i]\n",
        "        time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "        distance = geodesic(\n",
        "            (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "            (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "        ).meters\n",
        "        if time_diff > time_threshold or distance > distance_threshold:\n",
        "            anomalies.append({\n",
        "                \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "                \"timestamp_prev\": row_prev[\"timestamp\"],\n",
        "                \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "                \"time_diff_sec\": time_diff,\n",
        "                \"distance_m\": distance,\n",
        "                \"is_gap\": time_diff > time_threshold,\n",
        "                \"is_jump\": distance > distance_threshold,\n",
        "                \"anomaly_type\": \"jump_or_gap\"\n",
        "            })\n",
        "    return anomalies\n"
      ],
      "metadata": {
        "id": "t_6sx5MqR9M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 2. Detect Stuck Vehicles\n",
        "def detect_stuck_vehicle(df, speed_thresh=1.0, window=4):\n",
        "    stuck_flags = (\n",
        "        (df[\"speed_mph\"].rolling(window).mean() < speed_thresh) &\n",
        "        (df[\"latitude\"].diff().abs().rolling(window).mean() < 0.0001) &\n",
        "        (df[\"longitude\"].diff().abs().rolling(window).mean() < 0.0001)\n",
        "    )\n",
        "    return df[stuck_flags.fillna(False)].assign(anomaly_type=\"stuck_vehicle\")\n"
      ],
      "metadata": {
        "id": "ALyD6GfXSxcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 3. Detect Impossible Speeds\n",
        "def detect_impossible_speeds(df, speed_limit_kph=120):\n",
        "    records = []\n",
        "    for i in range(1, len(df)):\n",
        "        row_prev, row_curr = df.iloc[i - 1], df.iloc[i]\n",
        "        time_diff = (row_curr[\"timestamp\"] - row_prev[\"timestamp\"]).total_seconds()\n",
        "        if time_diff == 0:\n",
        "            continue\n",
        "        distance = geodesic(\n",
        "            (row_prev[\"latitude\"], row_prev[\"longitude\"]),\n",
        "            (row_curr[\"latitude\"], row_curr[\"longitude\"])\n",
        "        ).meters\n",
        "        speed_kph = (distance / time_diff) * 3.6\n",
        "        if speed_kph > speed_limit_kph:\n",
        "            records.append({\n",
        "                \"vehicle_id\": row_curr[\"vehicle_id\"],\n",
        "                \"timestamp_curr\": row_curr[\"timestamp\"],\n",
        "                \"computed_speed_kph\": speed_kph,\n",
        "                \"distance_m\": distance,\n",
        "                \"anomaly_type\": \"impossible_speed\"\n",
        "            })\n",
        "    return pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "ndEIWLFiS-KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 4. Detect Backtracking (Heading Reversal)\n",
        "def detect_backtracking(df, reversal_thresh=160):\n",
        "    backtrack_flags = df[\"heading\"].diff().abs().between(reversal_thresh, 200)\n",
        "    return df[backtrack_flags.fillna(False)].assign(anomaly_type=\"backtracking\")\n"
      ],
      "metadata": {
        "id": "qqXHFtQxTDsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 5. Detect Repeated Points\n",
        "def detect_repeated_points(df):\n",
        "    repeated = (\n",
        "        (df[\"latitude\"].diff().abs() < 1e-5) &\n",
        "        (df[\"longitude\"].diff().abs() < 1e-5)\n",
        "    )\n",
        "    return df[repeated.fillna(False)].assign(anomaly_type=\"repeated_points\")\n"
      ],
      "metadata": {
        "id": "qzLC3OeNTHtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 6. Detect Disappearance Without Return\n",
        "def detect_disappeared(df, snapshot_end_time, min_gap_minutes=10):\n",
        "    last_seen = df[\"timestamp\"].max()\n",
        "    if (snapshot_end_time - last_seen).total_seconds() > min_gap_minutes * 60:\n",
        "        return pd.DataFrame([{\n",
        "            \"vehicle_id\": df[\"vehicle_id\"].iloc[0],\n",
        "            \"last_seen\": last_seen,\n",
        "            \"anomaly_type\": \"disappearance\"\n",
        "        }])\n",
        "    return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "7OMIPA39TKb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 7. Detect Early Appearance\n",
        "def detect_early_appearance(df, snapshot_start_time, margin_seconds=30):\n",
        "    first_seen = df[\"timestamp\"].min()\n",
        "    if (first_seen - snapshot_start_time).total_seconds() < margin_seconds:\n",
        "        return pd.DataFrame([{\n",
        "            \"vehicle_id\": df[\"vehicle_id\"].iloc[0],\n",
        "            \"first_seen\": first_seen,\n",
        "            \"anomaly_type\": \"early_appearance\"\n",
        "        }])\n",
        "    return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "Z8lrciKnTNv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 8. Detect Off-Route Movement\n",
        "#1. Preprocess Route Shapes\n",
        "\n",
        "# Load and group shape points\n",
        "shapes = pd.read_csv(\"/content/gtfs_static/shapes.txt\")\n",
        "shapes[\"shape_id\"] = shapes[\"shape_id\"].astype(str)\n",
        "\n",
        "# Build LineStrings for each shape_id\n",
        "shape_lines = {}\n",
        "for shape_id, group in shapes.groupby(\"shape_id\"):\n",
        "    sorted_group = group.sort_values(\"shape_pt_sequence\")\n",
        "    coords = list(zip(sorted_group[\"shape_pt_lon\"], sorted_group[\"shape_pt_lat\"]))\n",
        "    shape_lines[shape_id] = LineString(coords)\n",
        "\n",
        "\n",
        "#2. Detect Off-Route for Each Vehicle Point\n",
        "#We check if the GPS point is >50m from its assigned shape line.\n",
        "\n",
        "def detect_off_route(df_vehicle, shape_lines, buffer_m=50):\n",
        "    records = []\n",
        "    for _, row in df_vehicle.iterrows():\n",
        "        shape_id = str(row.get(\"shape_id\"))\n",
        "        if shape_id not in shape_lines:\n",
        "            continue  # shape not known\n",
        "\n",
        "        route_line = shape_lines[shape_id]\n",
        "        vehicle_point = Point(row[\"longitude\"], row[\"latitude\"])\n",
        "\n",
        "        # Find closest point on route and compute geodesic distance\n",
        "        closest_point = route_line.interpolate(route_line.project(vehicle_point))\n",
        "        dist_m = geodesic(\n",
        "            (row[\"latitude\"], row[\"longitude\"]),\n",
        "            (closest_point.y, closest_point.x)\n",
        "        ).meters\n",
        "\n",
        "        if dist_m > buffer_m:\n",
        "            records.append({\n",
        "              \"vehicle_id\": row[\"vehicle_id\"],\n",
        "              \"timestamp\": row[\"timestamp\"],\n",
        "              \"route_short_name\": row.get(\"route_short_name\"),\n",
        "              \"distance_from_route_m\": dist_m,\n",
        "              \"latitude\": row[\"latitude\"],\n",
        "              \"longitude\": row[\"longitude\"],\n",
        "              \"shape_id\": shape_id,\n",
        "              \"anomaly_type\": \"off_route\"\n",
        "            })\n",
        "    return pd.DataFrame(records)\n",
        "\n"
      ],
      "metadata": {
        "id": "j7aJKusHTx6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bring together all Anomalies\n",
        "\n",
        "# Step 1: Normalize timestamp and rebuild vehicle groups\n",
        "df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "# Step 2: Detect each anomaly type\n",
        "\n",
        "jumpgap_records = []\n",
        "stuck_records = []\n",
        "speed_records = []\n",
        "backtrack_records = []\n",
        "repeated_records = []\n",
        "disappear_records = []\n",
        "early_records = []\n",
        "offroute_records = []\n",
        "\n",
        "snapshot_start = df_valid[\"timestamp\"].min()\n",
        "snapshot_end = df_valid[\"timestamp\"].max()\n",
        "\n",
        "for vehicle_id, df_vehicle in vehicle_groups.items():\n",
        "    df_vehicle = df_vehicle.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "    # 1. Jumps and Gaps\n",
        "    jumpgap_records.extend(detect_jumps_and_gaps(df_vehicle))\n",
        "\n",
        "    # 2. Stuck Vehicles\n",
        "    stuck = detect_stuck_vehicle(df_vehicle)\n",
        "    if not stuck.empty:\n",
        "        stuck_records.append(stuck)\n",
        "\n",
        "    # 3. Impossible Speeds\n",
        "    speed = detect_impossible_speeds(df_vehicle)\n",
        "    if not speed.empty:\n",
        "        speed_records.append(speed)\n",
        "\n",
        "    # 4. Backtracking\n",
        "    backtrack = detect_backtracking(df_vehicle)\n",
        "    if not backtrack.empty:\n",
        "        backtrack_records.append(backtrack)\n",
        "\n",
        "    # 5. Repeated Points\n",
        "    repeat = detect_repeated_points(df_vehicle)\n",
        "    if not repeat.empty:\n",
        "        repeated_records.append(repeat)\n",
        "\n",
        "    # 6. Disappearance\n",
        "    disappear = detect_disappeared(df_vehicle, snapshot_end)\n",
        "    if not disappear.empty:\n",
        "        disappear_records.append(disappear)\n",
        "\n",
        "    # 7. Early Appearance\n",
        "    early = detect_early_appearance(df_vehicle, snapshot_start)\n",
        "    if not early.empty:\n",
        "        early_records.append(early)\n",
        "\n",
        "    # 8. Off-Route\n",
        "    offroute = detect_off_route(df_vehicle, shape_lines)\n",
        "    if not offroute.empty:\n",
        "        offroute_records.append(offroute)\n",
        "\n",
        "# Step 3: Combine to DataFrames\n",
        "df_anomalies_jumpgap   = pd.DataFrame(jumpgap_records)\n",
        "df_anomalies_stuck     = pd.concat(stuck_records, ignore_index=True) if stuck_records else pd.DataFrame()\n",
        "df_anomalies_speed     = pd.concat(speed_records, ignore_index=True) if speed_records else pd.DataFrame()\n",
        "df_anomalies_backtrack = pd.concat(backtrack_records, ignore_index=True) if backtrack_records else pd.DataFrame()\n",
        "df_anomalies_repeated  = pd.concat(repeated_records, ignore_index=True) if repeated_records else pd.DataFrame()\n",
        "df_anomalies_disappear = pd.concat(disappear_records, ignore_index=True) if disappear_records else pd.DataFrame()\n",
        "df_anomalies_early     = pd.concat(early_records, ignore_index=True) if early_records else pd.DataFrame()\n",
        "df_anomalies_offroute  = pd.concat(offroute_records, ignore_index=True) if offroute_records else pd.DataFrame()\n",
        "\n",
        "# Step 4: Combine all anomalies into a single DataFrame\n",
        "anomaly_frames = [\n",
        "    df_anomalies_jumpgap,\n",
        "    df_anomalies_stuck,\n",
        "    df_anomalies_speed,\n",
        "    df_anomalies_backtrack,\n",
        "    df_anomalies_repeated,\n",
        "    df_anomalies_disappear,\n",
        "    df_anomalies_early,\n",
        "    df_anomalies_offroute\n",
        "]\n",
        "\n",
        "anomaly_frames = [df for df in anomaly_frames if 'anomaly_type' in df.columns and not df.empty]\n",
        "df_anomalies_full = pd.concat(anomaly_frames, ignore_index=True)\n",
        "print(\"Unified anomaly count:\", len(df_anomalies_full))\n",
        "df_anomalies_full[\"anomaly_type\"].value_counts()\n"
      ],
      "metadata": {
        "id": "xtosg_TNYhfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure shape_id is string type\n",
        "df_anomalies_offroute[\"shape_id\"] = df_anomalies_offroute[\"shape_id\"].astype(str)\n",
        "\n",
        "# Join to enrich anomalies with route info\n",
        "df_anomalies_offroute = (\n",
        "    df_anomalies_offroute\n",
        "    .merge(shape_route_map, on=\"shape_id\", how=\"left\", suffixes=(\"\", \"_from_map\"))\n",
        ")\n",
        "\n",
        "# If route_short_name was missing, replace it\n",
        "df_anomalies_offroute[\"route_short_name\"] = (\n",
        "    df_anomalies_offroute[\"route_short_name\"]\n",
        "    .fillna(df_anomalies_offroute[\"route_short_name_from_map\"])\n",
        ")\n",
        "\n",
        "# Drop helper column\n",
        "df_anomalies_offroute = df_anomalies_offroute.drop(columns=[\"route_short_name_from_map\"])\n"
      ],
      "metadata": {
        "id": "YcRHtnYJcDmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity Check\n",
        "missing_routes = df_anomalies_offroute[\"route_short_name\"].isnull().sum()\n",
        "print(f\"Remaining anomalies with missing route_short_name: {missing_routes}\")\n",
        "#If this prints 0, youâ€™ve successfully patched all entries."
      ],
      "metadata": {
        "id": "vEr-NnroWCUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Not sure if this is needed\n",
        "# #Assuming your vehicle groups are stored like:\n",
        "# vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "# #And youâ€™re aggregating anomalies like:\n",
        "# all_anomalies = []\n",
        "\n",
        "# #Add off-route detection per vehicle:\n",
        "# for vehicle_id, df_vehicle in vehicle_groups.items():\n",
        "#     df_vehicle = df_vehicle.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "#     # Call anomaly modules\n",
        "#     off_route_df = detect_off_route(df_vehicle, shape_lines)\n",
        "\n",
        "#     # Append results\n",
        "#     if not off_route_df.empty:\n",
        "#         all_anomalies.append(off_route_df)\n",
        "\n",
        "# # Final result\n",
        "# df_anomalies_offroute = pd.concat(all_anomalies, ignore_index=True)\n",
        "\n",
        "# #Optional Check:\n",
        "# print(\"Off-route anomalies detected:\", len(df_anomalies_offroute))\n",
        "# df_anomalies_offroute.sort_values(\"distance_from_route_m\", ascending=False).head()"
      ],
      "metadata": {
        "id": "FGw1HgFiVWLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 1.1 Filter for Valid Rows\n",
        "df_valid = df_full[df_full[\"data_quality\"] == \"valid\"].copy()\n",
        "\n",
        "#ðŸ”¹ 1.2 Parse Timestamps\n",
        "df_valid[\"timestamp\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], utc=True)\n",
        "\n",
        "#ðŸ”¹ 1.3 Sort by Vehicle and Timestamp\n",
        "df_valid = df_valid.sort_values(by=[\"vehicle_id\", \"timestamp\"])\n",
        "\n",
        "#ðŸ”¹ 1.4 Organize by Vehicle\n",
        "#This creates a dictionary keyed by vehicle ID, each with a sorted DataFrame:\n",
        "vehicle_groups = dict(tuple(df_valid.groupby(\"vehicle_id\")))\n",
        "\n",
        "#You can confirm how many distinct vehicles youâ€™re tracking:\n",
        "print(\"Vehicle count:\", len(vehicle_groups))\n"
      ],
      "metadata": {
        "id": "rjtc3efkP9GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary Stats by Route/Anomaly"
      ],
      "metadata": {
        "id": "_cUElQKNWSAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Patch route_short_name into df_valid (ensures downstream consistency)\n",
        "# Build trip-to-route lookup from your GTFS static data\n",
        "trip_to_route_lookup = trips_df[[\"trip_id\", \"route_id\"]].merge(\n",
        "    routes_df[[\"route_id\", \"route_short_name\"]], on=\"route_id\", how=\"left\"\n",
        ")\n",
        "\n",
        "#the data types of the trip_id column are mismatched between df_valid and trip_to_route_lookup\n",
        "df_valid[\"trip_id\"] = df_valid[\"trip_id\"].astype(str)\n",
        "trip_to_route_lookup[\"trip_id\"] = trip_to_route_lookup[\"trip_id\"].astype(str)\n",
        "\n",
        "# Patch into df_valid\n",
        "df_valid = df_valid.merge(trip_to_route_lookup, on=\"trip_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "JB51bYKw-ZGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Build a lookup table\n",
        "route_lookup = df_valid[[\"vehicle_id\", \"timestamp\", \"route_short_name\"]].copy()\n",
        "route_lookup[\"timestamp\"] = pd.to_datetime(route_lookup[\"timestamp\"], utc=True)\n"
      ],
      "metadata": {
        "id": "cRWfXaRS91JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Also convert timestamp in anomalies to datetime\n",
        "df_anomalies_full[\"timestamp\"] = pd.to_datetime(df_anomalies_full[\"timestamp\"], utc=True)\n"
      ],
      "metadata": {
        "id": "DYLHwfD3_w9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Join by vehicle and timestamp (merge nearest match within a short tolerance)\n",
        "# Sort before merge_asof\n",
        "route_lookup = route_lookup.sort_values([\"vehicle_id\", \"timestamp\"])\n",
        "df_anomalies_full = df_anomalies_full.sort_values([\"vehicle_id\", \"timestamp\"])\n",
        "\n",
        "#Filter out the rows with null timestamps before merging\n",
        "# Check how many nulls you have\n",
        "print(\"Null timestamps in anomalies:\", df_anomalies_full[\"timestamp\"].isna().sum())\n",
        "\n",
        "# Drop them before merge_asof (can't match anything anyway)\n",
        "df_anomalies_full = df_anomalies_full[df_anomalies_full[\"timestamp\"].notna()]\n",
        "\n",
        "# Enforce datetime dtype and full sort\n",
        "route_lookup[\"timestamp\"] = pd.to_datetime(route_lookup[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "df_anomalies_full[\"timestamp\"] = pd.to_datetime(df_anomalies_full[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "#Re-sort before the merge\n",
        "# Ensure sorting after dropping nulls\n",
        "route_lookup = route_lookup.sort_values(\"timestamp\")\n",
        "df_anomalies_full = df_anomalies_full.sort_values(\"timestamp\")\n",
        "\n",
        "\n",
        "# Merge with tolerance: 60 seconds\n",
        "df_anomalies_full = pd.merge_asof(\n",
        "    df_anomalies_full,\n",
        "    route_lookup,\n",
        "    by=\"vehicle_id\",\n",
        "    on=\"timestamp\",\n",
        "    direction=\"nearest\",\n",
        "    tolerance=pd.Timedelta(\"60s\"),\n",
        "    suffixes=(\"\", \"_from_lookup\")\n",
        ")\n",
        "\n",
        "# Final fallback\n",
        "df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].fillna(\n",
        "    df_anomalies_full[\"route_short_name_from_lookup\"]\n",
        ")\n",
        "\n",
        "print(\"Remaining missing route names:\", df_anomalies_full[\"route_short_name\"].isna().sum())"
      ],
      "metadata": {
        "id": "sHojTZAM_1Z4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treat 'Unknown' as null for fallback\n",
        "df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].replace(\"Unknown\", pd.NA)\n",
        "\n",
        "# Re-apply fallback\n",
        "df_anomalies_full[\"route_short_name\"] = df_anomalies_full[\"route_short_name\"].combine_first(\n",
        "    df_anomalies_full[\"route_short_name_from_lookup\"]\n",
        ")\n",
        "\n",
        "# Confirm\n",
        "print(\"Remaining missing route names:\", df_anomalies_full[\"route_short_name\"].isna().sum())\n",
        "print(df_anomalies_full[\"route_short_name\"].value_counts(dropna=False).head())\n"
      ],
      "metadata": {
        "id": "PGi473SoCNxC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_by_route_type = df_anomalies_full.pivot_table(\n",
        "    index=\"route_short_name\",\n",
        "    columns=\"anomaly_type\",\n",
        "    aggfunc=\"size\",\n",
        "    fill_value=0\n",
        ").sort_index()\n",
        "\n",
        "anomaly_by_route_type\n"
      ],
      "metadata": {
        "id": "sH1WbUABCncC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot in Folium"
      ],
      "metadata": {
        "id": "AaLiLOnGSB5J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ORsBn80ZXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Ensure latitude and longitude are floats\n",
        "df_valid[\"latitude\"] = pd.to_numeric(df_valid[\"latitude\"], errors=\"coerce\")\n",
        "df_valid[\"longitude\"] = pd.to_numeric(df_valid[\"longitude\"], errors=\"coerce\")\n",
        "\n",
        "#Step 2: Ensure timestamp_collected is datetime\n",
        "df_valid[\"timestamp_collected\"] = pd.to_datetime(df_valid[\"timestamp_collected\"], errors=\"coerce\", utc=True)\n"
      ],
      "metadata": {
        "id": "AAU6MySkUamy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the full cleaned dataframe\n",
        "df_full = df_valid.copy()\n",
        "\n",
        "# Compute diffs for position and time\n",
        "df_full[\"lat_diff\"] = df_full.groupby(\"vehicle_id\")[\"latitude\"].diff()\n",
        "df_full[\"lon_diff\"] = df_full.groupby(\"vehicle_id\")[\"longitude\"].diff()\n",
        "df_full[\"jump_dist\"] = (df_full[\"lat_diff\"]**2 + df_full[\"lon_diff\"]**2)**0.5\n",
        "\n",
        "df_full[\"time_diff\"] = df_full.groupby(\"vehicle_id\")[\"timestamp_collected\"].diff().dt.total_seconds()\n",
        "\n",
        "# Label events\n",
        "df_full[\"is_jump\"] = df_full[\"jump_dist\"] > JUMP_DISTANCE_THRESHOLD\n",
        "df_full[\"is_disappearance\"] = df_full[\"time_diff\"] > DISAPPEARANCE_TIME_THRESHOLD\n",
        "\n",
        "# Enrich jump rows with previous position\n",
        "jumps_df = df_full[df_full[\"is_jump\"]].copy()\n",
        "jumps_df[\"lat_prev\"] = df_full.groupby(\"vehicle_id\")[\"latitude\"].shift()\n",
        "jumps_df[\"lon_prev\"] = df_full.groupby(\"vehicle_id\")[\"longitude\"].shift()\n",
        "jumps_df[\"timestamp_prev\"] = df_full.groupby(\"vehicle_id\")[\"timestamp_collected\"].shift()\n",
        "jumps_df[\"timestamp_curr\"] = jumps_df[\"timestamp_collected\"]\n",
        "\n",
        "# Filter for vehicles with sufficient jumps\n",
        "jump_counts = jumps_df[\"vehicle_id\"].value_counts()\n",
        "keep_jumpers = jump_counts[jump_counts >= MIN_JUMP_COUNT_PER_VEHICLE].index\n",
        "jumps_df = jumps_df[jumps_df[\"vehicle_id\"].isin(keep_jumpers)]\n",
        "\n",
        "# Recalculate filtered set for map display\n",
        "df_jumpers_only = df_full[df_full[\"vehicle_id\"].isin(keep_jumpers)].copy()\n",
        "\n",
        "# Identify disappearance and reappearance points\n",
        "disappear_df = df_jumpers_only[df_jumpers_only[\"is_disappearance\"]].copy()\n",
        "# Shift the is_disappearance column and fill any resulting NaNs with False\n",
        "is_reappear = df_jumpers_only[\"is_disappearance\"].shift(-1).fillna(False)\n",
        "reappear_df = df_jumpers_only[is_reappear].copy()\n"
      ],
      "metadata": {
        "id": "2befHEepUgBs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is the full code to patch route_short_name into shapes_df, allowing each shape to be associated with its route for proper filtering in Folium:\n",
        "# Step 1: Extract shape_id to route_id mapping from trips_df\n",
        "shape_to_route = trips_df[[\"shape_id\", \"route_id\"]].drop_duplicates()\n",
        "\n",
        "# Step 2: Map route_id to route_short_name from routes_df\n",
        "route_id_to_name = routes_df[[\"route_id\", \"route_short_name\"]]\n",
        "\n",
        "# Step 3: Merge to associate shape_id with route_short_name\n",
        "shape_route_map = shape_to_route.merge(route_id_to_name, on=\"route_id\", how=\"left\")\n",
        "\n",
        "# Step 4: Merge back into shapes_df to add route_short_name column\n",
        "shapes_df = shapes_df.merge(shape_route_map, on=\"shape_id\", how=\"left\")\n",
        "\n",
        "# Step 5: Confirm result\n",
        "print(\"Sample of shapes_df with route_short_name:\")\n",
        "print(shapes_df[[\"shape_id\", \"route_id\", \"route_short_name\"]].drop_duplicates().head())\n",
        "\n",
        "#This ensures that each shape_id in your GTFS shapes.txt data now carries the corresponding route_short_name.\n",
        "#We can now group route shapes per route like this:\n",
        "#for route_name, group in shapes_df.groupby(\"route_short_name\"): ...\n",
        "\n"
      ],
      "metadata": {
        "id": "gDUGM5chVxrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create base map ---\n",
        "mymap = Map(location=[35.0844, -106.6504], zoom_start=12)\n",
        "\n",
        "# --- Define color map for route-based layers ---\n",
        "route_names = sorted(df_anomalies_full['route_short_name'].dropna().unique())\n",
        "cmap = plt.get_cmap(\"tab20\", len(route_names))\n",
        "color_map = {route: mcolors.to_hex(cmap(i)) for i, route in enumerate(route_names)}\n",
        "\n",
        "# --- Add anomaly points by (route, anomaly_type) ---\n",
        "for (route, anomaly), subset in df_anomalies_full.groupby([\"route_short_name\", \"anomaly_type\"]):\n",
        "    group = FeatureGroup(name=f\"{route} â€“ {anomaly}\", show=False)\n",
        "    for _, row in subset.iterrows():\n",
        "        popup_text = (\n",
        "            f\"Anomaly: {anomaly}<br>\"\n",
        "            f\"Route: {route}<br>\"\n",
        "            f\"Vehicle: {row.get('vehicle_id', 'N/A')}<br>\"\n",
        "            f\"Timestamp: {row.get('timestamp', 'N/A')}\"\n",
        "        )\n",
        "        CircleMarker(\n",
        "            location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "            radius=4,\n",
        "            color=color_map.get(route, \"black\"),\n",
        "            fill=True,\n",
        "            fill_opacity=0.9,\n",
        "            popup=popup_text\n",
        "        ).add_to(group)\n",
        "    group.add_to(mymap)\n",
        "\n",
        "# --- Add jump markers and lines by route ---\n",
        "if 'jumps_df' in globals() and not jumps_df.empty:\n",
        "    for route, group_df in jumps_df.groupby(\"route_short_name\"):\n",
        "        jump_line_group = FeatureGroup(name=f\"{route} â€“ Jump Lines\", show=False)\n",
        "        jump_point_group = FeatureGroup(name=f\"{route} â€“ Jump Start/End\", show=False)\n",
        "\n",
        "        for _, row in group_df.iterrows():\n",
        "            start = [row[\"lat_prev\"], row[\"lon_prev\"]]\n",
        "            end = [row[\"latitude\"], row[\"longitude\"]]\n",
        "            vehicle = row.get(\"vehicle_id\", \"N/A\")\n",
        "            t_prev = row.get(\"timestamp_prev\", \"N/A\")\n",
        "            t_curr = row.get(\"timestamp_curr\", \"N/A\")\n",
        "\n",
        "            PolyLine(\n",
        "                locations=[start, end],\n",
        "                color=\"orange\", weight=2,\n",
        "                tooltip=f\"Vehicle {vehicle} jump\"\n",
        "            ).add_to(jump_line_group)\n",
        "\n",
        "            CircleMarker(\n",
        "                location=start, radius=4, color=\"blue\", fill=True,\n",
        "                fill_opacity=0.9,\n",
        "                popup=f\"Vehicle {vehicle} START<br>{t_prev}\"\n",
        "            ).add_to(jump_point_group)\n",
        "\n",
        "            CircleMarker(\n",
        "                location=end, radius=4, color=\"purple\", fill=True,\n",
        "                fill_opacity=0.9,\n",
        "                popup=f\"Vehicle {vehicle} END<br>{t_curr}\"\n",
        "            ).add_to(jump_point_group)\n",
        "\n",
        "        jump_line_group.add_to(mymap)\n",
        "        jump_point_group.add_to(mymap)\n",
        "\n",
        "# --- Add disappearances and reappearances by route ---\n",
        "if 'disappear_df' in globals() and not disappear_df.empty:\n",
        "    for route, group_df in disappear_df.groupby(\"route_short_name\"):\n",
        "        disappear_group = FeatureGroup(name=f\"{route} â€“ Disappearances\", show=False)\n",
        "        for _, row in group_df.iterrows():\n",
        "            Marker(\n",
        "                location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "                icon=Icon(color=\"red\", icon=\"times-circle\", prefix=\"fa\"),\n",
        "                tooltip=f\"Vehicle {row.get('vehicle_id')} disappeared<br>{row.get('timestamp')}\"\n",
        "            ).add_to(disappear_group)\n",
        "        disappear_group.add_to(mymap)\n",
        "\n",
        "if 'reappear_df' in globals() and not reappear_df.empty:\n",
        "    for route, group_df in reappear_df.groupby(\"route_short_name\"):\n",
        "        reappear_group = FeatureGroup(name=f\"{route} â€“ Reappearances\", show=False)\n",
        "        for _, row in group_df.iterrows():\n",
        "            Marker(\n",
        "                location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "                icon=Icon(color=\"green\", icon=\"check-circle\", prefix=\"fa\"),\n",
        "                tooltip=f\"Vehicle {row.get('vehicle_id')} reappeared<br>{row.get('timestamp')}\"\n",
        "            ).add_to(reappear_group)\n",
        "        reappear_group.add_to(mymap)\n",
        "\n",
        "# --- Add route shapes as toggleable layers by route ---\n",
        "if 'shapes_df' in globals() and not shapes_df.empty:\n",
        "    for route, shape_group in shapes_df.groupby(\"route_short_name\"):\n",
        "        route_group = FeatureGroup(name=f\"{route} â€“ Route Shape\", show=False)\n",
        "        for shape_id, shape_data in shape_group.groupby(\"shape_id\"):\n",
        "            shape_data = shape_data.sort_values(\"shape_pt_sequence\")\n",
        "            latlons = list(zip(shape_data[\"shape_pt_lat\"], shape_data[\"shape_pt_lon\"]))\n",
        "            PolyLine(\n",
        "                locations=latlons,\n",
        "                color=color_map.get(route, \"gray\"),\n",
        "                weight=2,\n",
        "                opacity=0.6,\n",
        "                popup=f\"Route {route} | Shape {shape_id}\"\n",
        "            ).add_to(route_group)\n",
        "        route_group.add_to(mymap)\n",
        "\n",
        "# --- Add layer controls ---\n",
        "LayerControl(collapsed=False).add_to(mymap)\n",
        "\n",
        "# --- Display map ---\n",
        "mymap\n"
      ],
      "metadata": {
        "id": "B7zHH7uuW7R8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}